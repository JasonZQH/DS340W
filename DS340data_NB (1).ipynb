{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "goCDJEGi0hbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove empty rows"
      ],
      "metadata": {
        "id": "tNDX_E710u5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aKlcRfnbyg5C",
        "outputId": "37b368d1-4558-409f-d23c-c7fa7923e12d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0       Time                            data  label\n",
              "0           0  [00:12.55  Another day wasted out of time  Angry\n",
              "1           1  [00:14.69         I can't get out of this  Angry\n",
              "2           2  [00:16.03           Altered state of mind  Angry\n",
              "3           3  [00:17.41             I'm going overboard  Angry\n",
              "4           4  [00:18.75     My conscience meets decline  Angry"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3633f165-e7b7-4ad4-ab9f-ec87aded4fb8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Time</th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[00:12.55</td>\n",
              "      <td>Another day wasted out of time</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[00:14.69</td>\n",
              "      <td>I can't get out of this</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[00:16.03</td>\n",
              "      <td>Altered state of mind</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[00:17.41</td>\n",
              "      <td>I'm going overboard</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[00:18.75</td>\n",
              "      <td>My conscience meets decline</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3633f165-e7b7-4ad4-ab9f-ec87aded4fb8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3633f165-e7b7-4ad4-ab9f-ec87aded4fb8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3633f165-e7b7-4ad4-ab9f-ec87aded4fb8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "-h16WDBZI-Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06da1573-7f7b-4733-d84a-d584b9ff513a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20115"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(\n",
        "    axis=0,\n",
        "    how='any',\n",
        "    thresh=None,\n",
        "    subset=None,\n",
        "    inplace=True\n",
        ")"
      ],
      "metadata": {
        "id": "UxFem7pcKIZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "CZW9epV8KMyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf87e2e-d926-4abf-df72-bf6475932383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17674"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoder"
      ],
      "metadata": {
        "id": "Ik3GmPtUN8bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "X_train = df['data'].values \n",
        "\n",
        "y_train = df['label'].values\n",
        "\n",
        "print('before: %s ...' %y_train[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "print('after: %s ...' %y_train[:5])"
      ],
      "metadata": {
        "id": "k4YgBqqnOADp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72cf75c0-8b7f-497d-bbed-108a89dbb9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: ['Angry' 'Angry' 'Angry' 'Angry' 'Angry'] ...\n",
            "after: [0 0 0 0 0] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Porter Stemmer"
      ],
      "metadata": {
        "id": "0LBAbGqCKUZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
        "\n",
        "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
        "    \"\"\"\n",
        "    A Porter-Stemmer-Tokenizer hybrid to splits sentences into words (tokens) \n",
        "    and applies the porter stemming algorithm to each of the obtained token. \n",
        "    Tokens that are only consisting of punctuation characters are removed as well.\n",
        "    Only tokens that consist of more than one letter are being kept.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        \n",
        "    text : `str`. \n",
        "      A sentence that is to split into words.\n",
        "        \n",
        "    Returns\n",
        "    ----------\n",
        "    \n",
        "    no_punct : `str`. \n",
        "      A list of tokens after stemming and removing Sentence punctuation patterns.\n",
        "    \n",
        "    \"\"\"\n",
        "    lower_txt = text.lower()\n",
        "    tokens = nltk.wordpunct_tokenize(lower_txt)\n",
        "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
        "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
        "    return no_punct"
      ],
      "metadata": {
        "id": "h4AHJs1YGy1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_tokenizer(\"Don't !!! --- want swimming. \")"
      ],
      "metadata": {
        "id": "E5Ho-_irHAm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa5ac92-6889-4b66-fb21-a72c63e9f482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['don', 't', 'want', 'swim']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop words"
      ],
      "metadata": {
        "id": "6r1TfGWtKbRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stopwords.txt', 'r') as infile:\n",
        "    stop_words = infile.read().splitlines()\n",
        "print('stop words %s ...' %stop_words[:5])"
      ],
      "metadata": {
        "id": "XmbKHb96HJeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66f37a5-efca-4259-ec88-88212f21a673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop words [\"a's\", 'able', 'about', 'above', 'according'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Count Vectorizer"
      ],
      "metadata": {
        "id": "MelwczKbKg-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer(\n",
        "            encoding='utf-8',\n",
        "            decode_error='replace',\n",
        "            strip_accents='unicode',\n",
        "            analyzer='word',\n",
        "            binary=False,\n",
        "            stop_words=stop_words,\n",
        "            tokenizer=porter_tokenizer,\n",
        "            ngram_range=(1,1)\n",
        "    )"
      ],
      "metadata": {
        "id": "HLz51dYpIscX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"123 1 The\\n swimmer likes swimming so he swims. Don't didn`t\"]\n",
        "\n",
        "vec = vec.fit(vocab)\n",
        "\n",
        "sentence1 = vec.transform([u'The swimmer likes swimming.'])\n",
        "sentence2 = vec.transform(['The\\nswimmer \\nswims.'])\n",
        "\n",
        "\n",
        "print('TEST:')\n",
        "print('Vocabulary: %s' %vec.get_feature_names())\n",
        "print('Sentence 1: %s' %sentence1.toarray())\n",
        "print('Sentence 2: %s' %sentence2.toarray())"
      ],
      "metadata": {
        "id": "UIK_1rXIIwZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4fd57d-935a-4b18-98fb-7a2ba2987174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST:\n",
            "Vocabulary: ['didn', 'don', 'swim', 'swimmer', 't']\n",
            "Sentence 1: [[0 0 1 1 0]]\n",
            "Sentence 2: [[0 0 1 1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df['data'].values \n",
        "vec = vec.fit(X_train.ravel())\n",
        "print('Vocabulary size: %s' %len(vec.get_feature_names()))"
      ],
      "metadata": {
        "id": "K5rq-Qz9LFIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e7c0f4-d3c1-4137-e35e-367182913272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 4333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(\n",
        "            encoding='utf-8',\n",
        "            decode_error='replace',\n",
        "            strip_accents='unicode',\n",
        "            analyzer='word',\n",
        "            binary=False,\n",
        "            stop_words=stop_words,\n",
        "            tokenizer=porter_tokenizer,\n",
        "            ngram_range=(2,2)\n",
        "    )\n",
        "# N-grams = 2\n",
        "vocab = [\"123 1 The\\n swimmer likes swimming so he swims. Don't didn`t\"]\n",
        "\n",
        "vec = vec.fit(vocab)\n",
        "\n",
        "sentence1 = vec.transform([u'The swimmer likes swimming.'])\n",
        "sentence2 = vec.transform(['The\\nswimmer \\nswims.'])\n",
        "\n",
        "\n",
        "print('TEST:')\n",
        "print('Vocabulary: %s' %vec.get_feature_names())\n",
        "print('Sentence 1: %s' %sentence1.toarray())\n",
        "print('Sentence 2: %s' %sentence2.toarray())"
      ],
      "metadata": {
        "id": "WsdZVPEvLkNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbd19a0-0633-4f93-be51-8a289c7415d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST:\n",
            "Vocabulary: ['didn t', 'don t', 'swim don', 'swim swim', 'swimmer swim', 't didn']\n",
            "Sentence 1: [[0 0 0 0 1 0]]\n",
            "Sentence 2: [[0 0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tfidf Vectorizer"
      ],
      "metadata": {
        "id": "tYjAMI76MDFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "            encoding='utf-8',\n",
        "            decode_error='replace',\n",
        "            strip_accents='unicode',\n",
        "            analyzer='word',\n",
        "            binary=False,\n",
        "            stop_words=stop_words,\n",
        "            tokenizer=porter_tokenizer\n",
        "    )"
      ],
      "metadata": {
        "id": "k-PHQAdMMErS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"123 1 The\\n swimmer likes swimming so he swims. Don't didn`t\"]\n",
        "\n",
        "tfidf = tfidf.fit(vocab)\n",
        "\n",
        "sentence1 = tfidf.transform([u'The swimmer likes swimming.'])\n",
        "sentence2 = tfidf.transform(['The\\nswimmer \\nswims.'])\n",
        "\n",
        "\n",
        "print('TEST:')\n",
        "print('Vocabulary: %s' %tfidf.get_feature_names())\n",
        "print('Sentence 1: %s' %sentence1.toarray())\n",
        "print('Sentence 2: %s' %sentence2.toarray())"
      ],
      "metadata": {
        "id": "SUP6noepMIxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2332b432-98cc-4e6f-921b-1108d4955e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST:\n",
            "Vocabulary: ['didn', 'don', 'swim', 'swimmer', 't']\n",
            "Sentence 1: [[0.         0.         0.70710678 0.70710678 0.        ]]\n",
            "Sentence 2: [[0.         0.         0.70710678 0.70710678 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = tfidf.fit(X_train.ravel())\n",
        "\n",
        "print('Vocabulary size: %s' %len(tfidf.get_feature_names()))"
      ],
      "metadata": {
        "id": "e3FLosY1MNxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f489a33f-267f-4191-a136-3bbbf624b497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 4333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf.get_feature_names())"
      ],
      "metadata": {
        "id": "l1_WLGHSID7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c936f9e-ba99-49ad-e757-1f1af57907ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'aaa', 'aaaaaaah', 'aaaaahhhh', 'aaaaahhhhh', 'aaahhh', 'abandon', 'abl', 'abort', 'abound', 'abouta', 'abov', 'absent', 'absolut', 'absorb', 'abstract', 'absurd', 'abus', 'accent', 'accept', 'accus', 'ach', 'achiev', 'achin', 'acid', 'ackowledg', 'acquir', 'act', 'actin', 'action', 'actual', 'ad', 'add', 'addict', 'address', 'adelita', 'adio', 'admir', 'admit', 'admittedli', 'adolesc', 'ador', 'adrenalin', 'advanc', 'advantag', 'adventur', 'advic', 'aero', 'aeroplan', 'aerorplan', 'affect', 'afford', 'afraid', 'afterglow', 'afternoon', 'age', 'agenda', 'aggrav', 'aggress', 'ago', 'agoni', 'agre', 'ah', 'ahahahahah', 'ahead', 'ahh', 'ahhh', 'ahhhh', 'aid', 'aight', 'aim', 'ain', 'aint', 'air', 'airplan', 'ak', 'al', 'alaina', 'alanti', 'alarm', 'album', 'alcohol', 'alien', 'alison', 'aliv', 'alla', 'alley', 'alli', 'allright', 'alon', 'alongsid', 'alreadi', 'alright', 'alrightwith', 'alter', 'alway', 'amaz', 'amazingb', 'ambul', 'amen', 'america', 'american', 'amidst', 'ammunit', 'amo', 'amoxacillin', 'amplifi', 'amus', 'analyz', 'anathema', 'ancient', 'andr', 'angel', 'anger', 'angri', 'anguish', 'ani', 'anim', 'anni', 'annihil', 'announc', 'annoy', 'annoyin', 'anoth', 'answer', 'anthem', 'anti', 'antic', 'anu', 'anxiou', 'anybodi', 'anyday', 'anymor', 'anyon', 'anyth', 'anywher', 'apathi', 'apiec', 'apollo', 'apolog', 'apologis', 'appart', 'appeal', 'appetit', 'appli', 'appreci', 'approach', 'aqua', 'arachnophobian', 'arch', 'areason', 'aren', 'arf', 'argu', 'aris', 'arizona', 'ark', 'arm', 'armageddon', 'armband', 'art', 'artist', 'ash', 'asham', 'asid', 'asleep', 'asphyxi', 'ass', 'assault', 'asshol', 'assimil', 'associ', 'astray', 'astronomi', 'atarashii', 'atlant', 'atmospher', 'atom', 'atroc', 'attack', 'attent', 'attic', 'attorney', 'attract', 'audit', 'aug', 'aunt', 'autograph', 'automat', 'autumn', 'avail', 'avenu', 'avoid', 'aw', 'await', 'awak', 'awaken', 'award', 'awe', 'awhil', 'awoken', 'awwright', 'ay', 'azayaka', 'b', 'babe', 'babi', 'back', 'backstreet', 'backward', 'bad', 'baddest', 'badli', 'baffl', 'bag', 'balanc', 'balconi', 'ball', 'ballet', 'balloon', 'ballot', 'banana', 'band', 'bang', 'bank', 'bankhead', 'banshe', 'bar', 'barbar', 'barbqu', 'barcelona', 'bare', 'bark', 'barkin', 'barmaid', 'barrel', 'barter', 'base', 'basement', 'basic', 'bastard', 'bat', 'bath', 'bathroom', 'batter', 'batteri', 'battl', 'battlin', 'bauxit', 'bay', 'beach', 'beam', 'beamin', 'bean', 'bear', 'beast', 'beat', 'beati', 'beau', 'beauti', 'becam', 'becaus', 'becom', 'bed', 'bedroom', 'bee', 'beef', 'beefin', 'beenlov', 'beer', 'befor', 'beg', 'beggin', 'begin', 'beginnin', 'begun', 'behav', 'beheld', 'bein', 'belief', 'believ', 'bell', 'belong', 'belov', 'belt', 'bend', 'beneath', 'bennet', 'bent', 'benz', 'benzino', 'benzodiazapen', 'besid', 'bestest', 'bet', 'betcha', 'betray', 'bg', 'bi', 'bibl', 'bide', 'big', 'bigger', 'biggi', 'bike', 'billi', 'billion', 'binocular', 'bipolar', 'bird', 'birmingham', 'birth', 'birthday', 'bit', 'bitch', 'bitchin', 'bite', 'bitten', 'bitter', 'bizkit', 'bizzar', 'bizzel', 'bizzi', 'black', 'blame', 'blank', 'blanket', 'blankli', 'blaow', 'blast', 'blaster', 'blastin', 'blead', 'bled', 'bleed', 'blend', 'blender', 'bless', 'blew', 'blind', 'blindin', 'blindli', 'blink', 'bliss', 'bloah', 'block', 'blog', 'blond', 'blood', 'bloodi', 'bloodsh', 'bloodstain', 'bloom', 'blot', 'blous', 'blow', 'blowin', 'blown', 'blue', 'bluebird', 'bluemari', 'bluer', 'blunt', 'blush', 'board', 'boat', 'bobbi', 'bodi', 'boil', 'bold', 'bolt', 'bomb', 'bond', 'bone', 'bongo', 'bonita', 'boo', 'book', 'bookmark', 'boom', 'boooooommmmmm', 'boot', 'booz', 'bop', 'border', 'bore', 'borin', 'born', 'boss', 'bother', 'botherin', 'bottl', 'bought', 'boul', 'bouncer', 'bound', 'boundari', 'bountyland', 'bout', 'bow', 'bowel', 'boweri', 'bowl', 'box', 'boxer', 'boy', 'boyfriend', 'brace', 'bracket', 'braill', 'brain', 'brainwash', 'brake', 'branch', 'brand', 'brandi', 'brando', 'brat', 'brave', 'braveoth', 'brawl', 'bread', 'break', 'breakdown', 'breakfast', 'breakin', 'breast', 'breath', 'breathin', 'breed', 'breez', 'breezi', 'bress', 'brew', 'brick', 'bride', 'bridg', 'brigad', 'brigg', 'bright', 'brighter', 'brightli', 'brighton', 'brim', 'briney', 'bring', 'bringin', 'britney', 'bro', 'broke', 'broken', 'bronx', 'bronxi', 'brook', 'brooklyn', 'brotha', 'brother', 'brought', 'brown', 'brownston', 'bruis', 'bruisi', 'brush', 'bu', 'bubbl', 'buddha', 'buddi', 'bug', 'buggin', 'build', 'builign', 'built', 'bulgin', 'bullet', 'bulletproof', 'bulli', 'bullshit', 'bum', 'bun', 'bunch', 'bunni', 'burden', 'buri', 'burn', 'burnin', 'burnout', 'burnt', 'burst', 'buse', 'bush', 'busi', 'busier', 'bust', 'bustin', 'butcher', 'butterfli', 'button', 'buy', 'buyin', 'buzz', 'bye', 'c', 'cabin', 'cabl', 'cadav', 'cage', 'cake', 'cali', 'california', 'call', 'callin', 'calm', 'calvari', 'cam', 'camera', 'camino', 'cancer', 'candi', 'candl', 'candyman', 'cannonbal', 'cap', 'cappin', 'captain', 'captur', 'car', 'carcass', 'card', 'cardboard', 'care', 'career', 'careless', 'carelessli', 'caribo', 'carlo', 'carniv', 'carri', 'cartman', 'cartoon', 'carv', 'carzi', 'case', 'cash', 'casket', 'cast', 'caster', 'castl', 'casual', 'casualti', 'cat', 'catch', 'catchin', 'cathedr', 'cathol', 'caught', 'caus', 'cave', 'cavern', 'cayman', 'cd', 'ceil', 'celebr', 'cell', 'cello', 'cemeteri', 'center', 'centuri', 'cept', 'cereal', 'certainli', 'cess', 'chab', 'chain', 'chainsaw', 'chair', 'chamber', 'champagn', 'champaign', 'chanc', 'chandeli', 'chang', 'changin', 'chao', 'chap', 'chapter', 'charcoal', 'chariti', 'charlott', 'charm', 'chart', 'chase', 'chat', 'chatter', 'cheap', 'cheat', 'check', 'checkin', 'checkmat', 'cheek', 'cheer', 'chelsea', 'chemo', 'cheng', 'chere', 'cherri', 'chest', 'chevrolet', 'chew', 'chicaneri', 'chick', 'chicken', 'child', 'childhood', 'childish', 'children', 'chill', 'chimney', 'china', 'chines', 'chloe', 'chocolatey', 'choic', 'choir', 'choke', 'chokin', 'choo', 'choos', 'chop', 'chord', 'choru', 'chose', 'chosen', 'chri', 'christ', 'christian', 'christina', 'christma', 'christoph', 'chrome', 'chump', 'church', 'ci', 'cicada', 'cigarett', 'cinco', 'cinder', 'cinema', 'cinnamon', 'circl', 'circu', 'citi', 'civil', 'ck', 'clack', 'claim', 'claimin', 'clap', 'clarksvil', 'clashin', 'class', 'classroom', 'clay', 'clean', 'cleanin', 'cleanli', 'clear', 'clearer', 'clearli', 'clench', 'clever', 'clich', 'click', 'climb', 'clip', 'clock', 'clockin', 'close', 'closer', 'closest', 'closet', 'cloth', 'cloud', 'cloudi', 'clown', 'club', 'cluck', 'clue', 'clueless', 'clutter', 'coal', 'coast', 'coat', 'cobe', 'cocain', 'cock', 'coffe', 'coffin', 'coke', 'colbi', 'cold', 'colder', 'coldest', 'collaps', 'collar', 'collect', 'collid', 'collis', 'cologn', 'color', 'colour', 'columbin', 'coma', 'comatos', 'comb', 'combin', 'combust', 'comcast', 'comeback', 'comedi', 'comfort', 'comin', 'command', 'commin', 'commod', 'common', 'commot', 'commun', 'compani', 'compar', 'compet', 'complain', 'complet', 'complic', 'compliment', 'compormis', 'compos', 'composur', 'compremis', 'compress', 'compromis', 'con', 'conceal', 'concentr', 'concern', 'concurr', 'concus', 'condescend', 'condit', 'condo', 'confess', 'confid', 'conflict', 'conform', 'confront', 'confus', 'connect', 'conquer', 'conscienc', 'conscious', 'constant', 'construct', 'consum', 'contact', 'contempl', 'contempt', 'contortionist', 'contract', 'contribut', 'control', 'conveni', 'convers', 'convict', 'convinc', 'cook', 'cooki', 'cookin', 'cool', 'cooler', 'cop', 'cope', 'copi', 'copper', 'coppin', 'cord', 'corner', 'corpor', 'corps', 'correct', 'cost', 'costum', 'couch', 'couldn', 'couldnt', 'counsel', 'count', 'countdown', 'counter', 'countri', 'coupl', 'courag', 'cours', 'court', 'cousin', 'cove', 'cover', 'cow', 'coward', 'cowboy', 'cower', 'coz', 'crack', 'cracker', 'craft', 'cramp', 'cranberri', 'crane', 'crap', 'crash', 'crashin', 'crater', 'crave', 'crawl', 'crawler', 'crawlin', 'craze', 'crazi', 'creak', 'cream', 'creat', 'credit', 'creep', 'creepi', 'creepin', 'crept', 'crew', 'cri', 'crib', 'crime', 'crimin', 'crinkl', 'crippl', 'crist', 'critic', 'crop', 'cross', 'crowd', 'crown', 'crucifi', 'crude', 'cruel', 'cruis', 'crumbl', 'crumblin', 'crush', 'cryin', 'crypt', 'crystal', 'cube', 'cuddli', 'cuddyli', 'cue', 'cum', 'cup', 'cupid', 'curb', 'cure', 'curios', 'curl', 'curs', 'curtain', 'cushion', 'cuss', 'cut', 'cute', 'cuz', 'cycl', 'cynic', 'cypher', 'cypress', 'd', 'da', 'dad', 'daddi', 'daft', 'dagger', 'daili', 'daisi', 'dake', 'dakedo', 'dakota', 'dalli', 'damag', 'dammit', 'damn', 'damnit', 'danc', 'dancefloor', 'dancer', 'danger', 'dangl', 'daniel', 'danni', 'dapper', 'dare', 'dark', 'darken', 'darker', 'darkest', 'darl', 'darlin', 'dash', 'dashboard', 'dat', 'date', 'daughter', 'davi', 'david', 'dawn', 'day', 'daydream', 'daylight', 'ddead', 'de', 'dead', 'deadbeat', 'deadli', 'deaf', 'deal', 'dealer', 'dear', 'death', 'debat', 'debri', 'debt', 'decay', 'deceit', 'deceiv', 'decemb', 'decid', 'deciev', 'decis', 'declin', 'dedic', 'dee', 'deep', 'deeper', 'deepest', 'defac', 'defenc', 'defend', 'defens', 'defi', 'deficit', 'defin', 'definit', 'deflat', 'degener', 'degre', 'deiti', 'delici', 'delight', 'delila', 'deliv', 'deliver', 'delus', 'delusion', 'demand', 'dementia', 'demon', 'demonstr', 'deni', 'denial', 'depend', 'deprav', 'depress', 'depth', 'derail', 'derrang', 'descant', 'descend', 'descent', 'describ', 'descript', 'desert', 'deserv', 'design', 'desir', 'desk', 'desper', 'destin', 'destini', 'destroy', 'destruct', 'detail', 'detroit', 'develop', 'deviat', 'devic', 'devil', 'devot', 'di', 'dial', 'diamond', 'diaper', 'diari', 'dice', 'dick', 'didn', 'didnt', 'die', 'diff', 'differ', 'difficult', 'dig', 'diggin', 'digit', 'digniti', 'dijo', 'dilli', 'dim', 'dine', 'dinner', 'direct', 'directli', 'dirt', 'dirti', 'disagre', 'disappear', 'disappoint', 'disapprov', 'disast', 'disc', 'disciplin', 'disco', 'disconnect', 'discont', 'discourag', 'discov', 'discreetli', 'discrimin', 'discuss', 'disdain', 'dise', 'diseas', 'diseveri', 'disfigur', 'disfunct', 'disgrac', 'disguis', 'disgust', 'dishonor', 'dismemb', 'dispel', 'disposit', 'disprov', 'disrespect', 'diss', 'dissapear', 'dissin', 'dissip', 'distanc', 'distant', 'distort', 'distress', 'ditch', 'dive', 'divid', 'divin', 'dizzi', 'dj', 'doberman', 'doctor', 'dodg', 'doe', 'doesn', 'doesnt', 'dog', 'doin', 'doll', 'dollar', 'dometh', 'domin', 'don', 'donna', 'dont', 'doodi', 'door', 'doorfram', 'doorway', 'dope', 'dopest', 'dot', 'doubl', 'doubt', 'dough', 'dove', 'dracula', 'draft', 'drag', 'dragon', 'drain', 'drama', 'drank', 'drastic', 'draw', 'drawer', 'drawn', 'dre', 'dread', 'dream', 'dreamer', 'dreami', 'dreamin', 'dreamt', 'dress', 'drew', 'dri', 'drift', 'drink', 'drinkin', 'drip', 'drippin', 'drive', 'driven', 'driver', 'driveway', 'drivin', 'drop', 'droppin', 'drove', 'drown', 'drug', 'drum', 'drunk', 'drunkard', 'drunken', 'dub', 'duck', 'duct', 'dude', 'due', 'duhhh', 'duke', 'dull', 'dum', 'dumb', 'dumbass', 'dumpster', 'duplic', 'dure', 'dust', 'dvd', 'dwellin', 'dy', 'dye', 'dyin', 'e', 'ear', 'earli', 'earn', 'earth', 'eas', 'easi', 'easier', 'easili', 'east', 'eat', 'eatin', 'echo', 'ecstasi', 'ecstat', 'edg', 'edibl', 'edit', 'educ', 'ee', 'eee', 'effect', 'egg', 'ego', 'egotist', 'eh', 'ehhhh', 'eiffel', 'eighti', 'ejacul', 'el', 'electr', 'eleg', 'elev', 'elizabethi', 'eloqu', 'els', 'em', 'embarrass', 'ember', 'embrac', 'emce', 'eminem', 'emot', 'empir', 'employe', 'empti', 'empty', 'enclosur', 'end', 'endless', 'enemi', 'energi', 'enforc', 'engag', 'engin', 'england', 'enjoy', 'enrag', 'entail', 'enter', 'entertain', 'entir', 'entitl', 'envi', 'envis', 'episod', 'epitaph', 'epitom', 'eras', 'erect', 'errat', 'escal', 'escap', 'ese', 'especi', 'estat', 'esteem', 'etern', 'european', 'eve', 'evenin', 'eventhough', 'everi', 'everlast', 'everlastin', 'everybodi', 'everyday', 'everyon', 'everyth', 'everythin', 'everytim', 'everyway', 'everywher', 'evid', 'evil', 'evolut', 'exactli', 'exalt', 'exampl', 'excel', 'exchang', 'excit', 'excpet', 'excus', 'exercis', 'exhaust', 'exist', 'exit', 'expand', 'expect', 'expectin', 'expens', 'expert', 'explain', 'explod', 'explodin', 'explor', 'expos', 'express', 'extend', 'extort', 'extraordinari', 'extrem', 'exus', 'eye', 'eyewit', 'f', 'fa', 'fabric', 'facad', 'face', 'faceless', 'fact', 'factori', 'fad', 'fade', 'fag', 'faggot', 'fahrenheit', 'fail', 'failur', 'fair', 'fairi', 'faith', 'faithless', 'fake', 'faker', 'falcon', 'fall', 'fallen', 'fallin', 'fame', 'famili', 'familiar', 'famou', 'fan', 'fanci', 'fantasi', 'farewel', 'farm', 'farther', 'fascin', 'fashion', 'fast', 'faster', 'fat', 'fate', 'father', 'fault', 'favor', 'favorit', 'favourit', 'faygo', 'fear', 'feather', 'featur', 'fed', 'feebl', 'feed', 'feel', 'feelin', 'feet', 'fell', 'fella', 'fellin', 'fellow', 'felt', 'femal', 'fiction', 'field', 'fiend', 'fierc', 'fifti', 'fight', 'fightclub', 'fightin', 'figur', 'file', 'fill', 'fillin', 'film', 'filth', 'final', 'find', 'fine', 'finest', 'finger', 'fingernail', 'fingertip', 'finish', 'fire', 'firecrack', 'firefli', 'fireplac', 'firework', 'fish', 'fishin', 'fist', 'fit', 'fitter', 'fix', 'fixat', 'fixtur', 'fizzin', 'flag', 'flame', 'flaquito', 'flare', 'flash', 'flashback', 'flat', 'flatter', 'flatteri', 'flavor', 'fled', 'flesh', 'flew', 'flex', 'fli', 'flicker', 'flight', 'fling', 'flinger', 'flip', 'flirtati', 'float', 'flock', 'flood', 'floor', 'flop', 'flow', 'flower', 'flowerhat', 'flown', 'flurri', 'flush', 'flute', 'flutter', 'flyin', 'fo', 'foam', 'focu', 'fodder', 'foe', 'fog', 'fold', 'folk', 'follow', 'fondest', 'food', 'fool', 'foolish', 'foot', 'footprint', 'footstep', 'forc', 'fore', 'foreign', 'forese', 'forest', 'forev', 'forget', 'forgettin', 'forgiv', 'forgiven', 'forgot', 'forgotten', 'form', 'formal', 'formul', 'forrest', 'forsak', 'forsaken', 'fortun', 'forward', 'fought', 'found', 'foundat', 'fountain', 'fourth', 'fox', 'foxtrot', 'fractur', 'fragil', 'fragment', 'frail', 'frame', 'frankli', 'fray', 'freak', 'freaki', 'fred', 'free', 'freedom', 'freeli', 'freez', 'freight', 'french', 'frenet', 'frequenc', 'fresco', 'fresh', 'freshli', 'fri', 'friday', 'fridg', 'friend', 'friendli', 'friendship', 'fright', 'frighten', 'frog', 'front', 'frown', 'frownin', 'frozen', 'fruit', 'frustrat', 'fuck', 'fucker', 'fuckhead', 'fuckin', 'fuel', 'fulfil', 'full', 'fulla', 'fume', 'fun', 'fune', 'funer', 'funni', 'furi', 'fuse', 'fuss', 'fussin', 'futur', 'fuzzi', 'g', 'ga', 'gag', 'gage', 'gain', 'game', 'gangsta', 'gangster', 'gape', 'garbag', 'garden', 'gasbag', 'gasolin', 'gasp', 'gass', 'gat', 'gate', 'gather', 'gaug', 'gave', 'gay', 'gaze', 'geeeyeah', 'geek', 'gees', 'geez', 'gener', 'genit', 'genocid', 'gentl', 'gentli', 'georgia', 'gepetto', 'gerbil', 'getaway', 'gettin', 'ghost', 'giant', 'gibbon', 'gibson', 'gift', 'gimm', 'gin', 'girl', 'girlfriend', 'girli', 'give', 'givin', 'glad', 'glamour', 'glanc', 'glare', 'glass', 'glide', 'glimps', 'globe', 'globetrott', 'glock', 'glori', 'glove', 'glow', 'goal', 'god', 'goddam', 'goddamn', 'godiva', 'godsend', 'godspe', 'goe', 'goin', 'gold', 'golden', 'gon', 'gonna', 'good', 'goodby', 'goodnick', 'goodnight', 'gooo', 'goooo', 'goos', 'gordon', 'goshen', 'gossam', 'gota', 'gotta', 'govern', 'grab', 'grace', 'graciou', 'grade', 'grafton', 'graham', 'grain', 'grand', 'grant', 'grasp', 'grass', 'grate', 'grave', 'gravel', 'graviti', 'gray', 'great', 'greater', 'greatest', 'greatli', 'greec', 'greed', 'green', 'greener', 'grew', 'grey', 'grief', 'griev', 'grievin', 'gril', 'grill', 'grim', 'grin', 'grind', 'grip', 'grit', 'groceri', 'gross', 'grotti', 'ground', 'group', 'groupi', 'grow', 'growin', 'grown', 'grrrr', 'grudg', 'gruesom', 'guarante', 'guard', 'gucci', 'gue', 'guess', 'guest', 'guid', 'guilt', 'guilti', 'gull', 'gun', 'gunna', 'gunpowd', 'gushin', 'gut', 'guy', 'h', 'ha', 'haa', 'habit', 'habitat', 'hadn', 'hahaa', 'hahaha', 'hahahahaa', 'hail', 'haili', 'hair', 'half', 'halfway', 'hall', 'hallelujah', 'hallow', 'hallucin', 'halo', 'ham', 'hammer', 'hand', 'handcuff', 'handl', 'handshak', 'handsom', 'handstand', 'hang', 'hangin', 'hap', 'happen', 'happenin', 'happi', 'happili', 'har', 'hard', 'harder', 'hardest', 'hardli', 'hare', 'harlow', 'harm', 'harsh', 'hat', 'hate', 'hater', 'hath', 'hatin', 'hatr', 'haunt', 'haven', 'havin', 'hay', 'haze', 'head', 'headach', 'headboard', 'headin', 'headlight', 'headlin', 'headstand', 'heal', 'healer', 'health', 'hear', 'heard', 'heart', 'heartach', 'heartbeat', 'heartbreak', 'heat', 'heaven', 'heavenli', 'heavi', 'hebrew', 'heed', 'heeeeey', 'heeeelp', 'heeeey', 'heeey', 'heel', 'heh', 'hehe', 'height', 'heil', 'heineken', 'held', 'hell', 'hen', 'heresi', 'hermaph', 'hero', 'hershey', 'hesit', 'heterophob', 'hey', 'heyyy', 'hid', 'hidden', 'hide', 'hideout', 'hidin', 'high', 'higher', 'highest', 'highlight', 'hikaru', 'hill', 'hint', 'hip', 'hire', 'histori', 'hit', 'hitchhik', 'hito', 'hittin', 'hmm', 'hmmm', 'ho', 'hock', 'hoe', 'hola', 'hold', 'holdin', 'hole', 'holi', 'holiday', 'holla', 'hollerin', 'hollow', 'home', 'homecom', 'homeless', 'homesick', 'homeward', 'homi', 'homicid', 'homophob', 'homosex', 'honest', 'honesti', 'honestli', 'honey', 'honor', 'hoo', 'hood', 'hooda', 'hoodlum', 'hoodrat', 'hook', 'hop', 'hope', 'hopeless', 'hopelessli', 'hoppin', 'horizon', 'horrif', 'hors', 'hose', 'hospit', 'host', 'hot', 'hotter', 'hour', 'hourli', 'hous', 'houseguest', 'howev', 'howl', 'howlin', 'hu', 'huf', 'huff', 'hug', 'huh', 'hull', 'hum', 'human', 'humbl', 'humili', 'humour', 'hundr', 'hunger', 'hungri', 'hunt', 'hurri', 'hurrican', 'hurt', 'hush', 'hustler', 'hydrant', 'hymn', 'hype', 'hypermarket', 'hypnosi', 'hypnot', 'hypton', 'hysteria', 'i', 'ice', 'id', 'idea', 'identifi', 'idiot', 'ignor', 'iku', 'ill', 'illin', 'illumin', 'illus', 'im', 'ima', 'imag', 'imageri', 'imagin', 'imaginari', 'imit', 'imma', 'immin', 'immol', 'immort', 'impair', 'implor', 'import', 'imposs', 'impress', 'imprison', 'improv', 'impuls', 'inbetween', 'incarn', 'inch', 'includ', 'incomplet', 'increasingli', 'incred', 'incub', 'inde', 'index', 'indian', 'indispens', 'individu', 'indoor', 'indulg', 'industri', 'inem', 'inevit', 'infami', 'infect', 'infer', 'infinit', 'influenc', 'inform', 'ing', 'ingredi', 'inhal', 'inherit', 'inhibit', 'ink', 'innoc', 'inori', 'input', 'insan', 'insect', 'insid', 'insomniac', 'inspect', 'inspir', 'instant', 'instinct', 'instrument', 'insubordin', 'insult', 'insur', 'intellig', 'intent', 'interact', 'interest', 'interlud', 'intermezzo', 'internet', 'interrupt', 'interst', 'intervent', 'interview', 'intox', 'introduc', 'introduct', 'intrud', 'intuit', 'invent', 'invest', 'invis', 'invit', 'ira', 'iri', 'irrat', 'irrespons', 'isl', 'isla', 'island', 'isn', 'isol', 'issu', 'ive', 'iwo', 'iyiyi', 'j', 'ja', 'jack', 'jacket', 'jada', 'jade', 'jag', 'jail', 'jailcel', 'jam', 'jamaica', 'jame', 'jar', 'jason', 'jaw', 'jay', 'jealou', 'jean', 'jennif', 'jerri', 'jerusalem', 'jester', 'jesu', 'jet', 'jewelleri', 'jewelri', 'jima', 'job', 'joey', 'jog', 'john', 'join', 'joke', 'joker', 'jokin', 'journey', 'joyrid', 'ju', 'juda', 'judg', 'judgement', 'juli', 'juliet', 'jump', 'junk', 'junki', 'justifi', 'k', 'kaniff', 'kany', 'kara', 'karma', 'kay', 'kch', 'ke', 'kedo', 'keepin', 'ken', 'kennedi', 'kenshin', 'kevin', 'key', 'keyboard', 'kick', 'kickin', 'kid', 'kill', 'killer', 'killin', 'kim', 'kind', 'kinda', 'kinder', 'king', 'kingdom', 'kingpin', 'kingston', 'kiss', 'kissin', 'kitchen', 'kitten', 'kitti', 'knee', 'kneel', 'knew', 'knife', 'knifin', 'knight', 'knive', 'knock', 'knockin', 'knowin', 'knuckl', 'kogi', 'kokoro', 'konw', 'kow', 'krauss', 'kufuffin', 'kurayami', 'kurupt', 'l', 'la', 'lab', 'label', 'lace', 'lack', 'ladder', 'ladi', 'laid', 'lake', 'lala', 'lamb', 'lame', 'land', 'landfil', 'landlord', 'lane', 'languag', 'laps', 'larger', 'larri', 'laser', 'lash', 'late', 'latest', 'latino', 'laugh', 'laughin', 'laughter', 'launchin', 'laundri', 'law', 'lay', 'layer', 'le', 'lead', 'leader', 'leafi', 'lean', 'leanin', 'leant', 'leap', 'learn', 'leather', 'leav', 'leavem', 'leavin', 'lectur', 'led', 'lee', 'left', 'leg', 'legend', 'leisur', 'lemm', 'lemon', 'lemonad', 'len', 'lend', 'length', 'lennon', 'lennox', 'leper', 'lesson', 'lethal', 'letter', 'lettin', 'lexington', 'lez', 'li', 'liar', 'liber', 'liberti', 'licenc', 'lick', 'lid', 'lie', 'life', 'lifetim', 'lift', 'ligament', 'light', 'lighter', 'lightli', 'lightn', 'lil', 'lili', 'limb', 'limit', 'limo', 'limousin', 'limp', 'lincoln', 'line', 'ling', 'linger', 'linkin', 'lion', 'lip', 'lipstick', 'liquid', 'liquor', 'lisa', 'list', 'listen', 'lit', 'liter', 'littl', 'littlest', 'live', 'livin', 'lke', 'll', 'llgo', 'load', 'lobbi', 'lobe', 'lock', 'lodg', 'lollipop', 'london', 'lone', 'loneli', 'long', 'longer', 'lookin', 'loom', 'loon', 'looney', 'loos', 'loosen', 'lopez', 'lord', 'lose', 'loser', 'loss', 'lost', 'lot', 'lotta', 'loud', 'louder', 'loudmouth', 'loui', 'love', 'lover', 'lovin', 'lovingli', 'low', 'lower', 'loyalti', 'lp', 'lt', 'luck', 'lucki', 'luckless', 'lullabi', 'lump', 'lunaci', 'lunch', 'lunchbox', 'lunchtim', 'lune', 'lung', 'lure', 'lust', 'lyin', 'lyric', 'm', 'ma', 'mac', 'machett', 'machin', 'mack', 'mad', 'mada', 'made', 'madmen', 'madonna', 'magazin', 'magic', 'magnect', 'mahito', 'maiden', 'mail', 'mailbox', 'main', 'major', 'make', 'maker', 'makin', 'male', 'malfunct', 'mall', 'malley', 'malt', 'mama', 'mamma', 'man', 'manag', 'mani', 'maniac', 'manson', 'manufactur', 'map', 'mar', 'marbl', 'march', 'mari', 'marin', 'mark', 'marlon', 'marmalad', 'maroon', 'marri', 'marshal', 'marshmallow', 'martin', 'martyr', 'marvel', 'mash', 'mask', 'mass', 'massacr', 'master', 'masterpiec', 'mata', 'match', 'matchbox', 'mate', 'materi', 'mather', 'matsu', 'matter', 'mattress', 'maxrnb', 'mayb', 'mayhem', 'mayor', 'maze', 'mc', 'mcmorrow', 'meadow', 'meal', 'meaning', 'meaningless', 'meant', 'meantim', 'meanwhil', 'measur', 'meat', 'mecca', 'media', 'medicin', 'mediocr', 'meet', 'meimei', 'mekakushi', 'mekhi', 'melodi', 'melon', 'melt', 'memori', 'men', 'mend', 'menrow', 'mental', 'mention', 'mentionin', 'merci', 'merri', 'mess', 'messag', 'messiah', 'met', 'metal', 'metaphor', 'methadon', 'method', 'mexican', 'mexico', 'mi', 'mic', 'middl', 'midnight', 'midst', 'mighti', 'migrat', 'mildew', 'mile', 'milk', 'milki', 'million', 'mind', 'mindfuck', 'mindless', 'mine', 'minim', 'minor', 'minut', 'miracl', 'mire', 'mirror', 'misconcept', 'miseri', 'misogynist', 'miss', 'missi', 'missin', 'mission', 'mistak', 'mister', 'misti', 'misunderstood', 'mitai', 'mix', 'mm', 'mma', 'mmm', 'mmmm', 'mmmmm', 'mmmmmmmmmmm', 'mo', 'mob', 'mobil', 'mock', 'model', 'modern', 'mold', 'mom', 'moment', 'mommi', 'mon', 'monday', 'mondo', 'money', 'monkey', 'monochrom', 'monopoli', 'monorail', 'monster', 'mont', 'month', 'monument', 'mood', 'moon', 'moonbeam', 'moonlight', 'moonlit', 'mop', 'moral', 'morn', 'morni', 'mornin', 'mortar', 'mortician', 'motha', 'mothafuckin', 'mother', 'motherfuck', 'motherfuckin', 'motion', 'motiv', 'motorbreath', 'mountain', 'mourn', 'mous', 'mouth', 'move', 'movi', 'movin', 'mr', 'ms', 'mtv', 'mudsling', 'mullet', 'multi', 'multipl', 'munchausen', 'mune', 'murder', 'muscl', 'museum', 'music', 'musn', 'musta', 'mustard', 'muthafuck', 'muthafucka', 'muthafuckaaaaaaaarrggghhhh', 'mutherfuck', 'mutherfuckin', 'mutil', 'muziqu', 'myslef', 'mysteri', 'myth', 'n', 'na', 'nada', 'nah', 'nail', 'nake', 'nameless', 'nami', 'nana', 'nananananana', 'nap', 'nappi', 'nappin', 'narrow', 'nate', 'nathan', 'nation', 'natta', 'natur', 'naugh', 'nauseou', 'nav', 'navi', 'nawt', 'ne', 'nearbi', 'neat', 'neath', 'neck', 'nectar', 'needl', 'needlework', 'nefari', 'neg', 'neighborhood', 'neighbour', 'neon', 'nerv', 'nervou', 'ness', 'net', 'neva', 'neverend', 'neverland', 'news', 'newsstand', 'ni', 'nibbl', 'nice', 'nigga', 'niggaz', 'night', 'nightclub', 'nightleft', 'nightmar', 'nile', 'ninio', 'nitro', 'nobodi', 'nois', 'noisi', 'nonsens', 'noodl', 'nooki', 'noon', 'noos', 'normal', 'norman', 'north', 'northern', 'nose', 'nostril', 'note', 'noth', 'nothin', 'notic', 'notion', 'notori', 'notta', 'nouvel', 'novemb', 'nowher', 'nude', 'nukedashitakunaru', 'numb', 'number', 'nun', 'nut', 'nuttin', 'o', 'oak', 'oar', 'obey', 'object', 'oblivion', 'obsess', 'obstacl', 'obviou', 'occupi', 'ocean', 'od', 'odayaka', 'odd', 'offenc', 'offend', 'offens', 'offer', 'offic', 'offici', 'ohh', 'ohhh', 'ohhhh', 'ohhhhh', 'ohhhhhh', 'ohhhhhhhhhhh', 'oin', 'oj', 'okey', 'ol', 'olb', 'older', 'ole', 'omoi', 'onc', 'onli', 'oo', 'ooh', 'ooo', 'oooh', 'ooohh', 'oooo', 'ooooh', 'oooooh', 'ooooooh', 'ooooooooo', 'oou', 'open', 'opera', 'opinion', 'opportun', 'opportunity', 'opposit', 'oppress', 'optimist', 'option', 'oral', 'orang', 'orchestra', 'orchid', 'order', 'ordinari', 'origin', 'orion', 'orlean', 'orship', 'otherwis', 'oughta', 'ourselv', 'outcom', 'outdrew', 'outfit', 'outkast', 'outlaw', 'outsid', 'outta', 'outweigh', 'oval', 'oven', 'overboard', 'overcom', 'overcompens', 'overdos', 'overgrown', 'overheard', 'overload', 'overr', 'overstay', 'overthrew', 'overtim', 'ow', 'owe', 'owner', 'ox', 'oxford', 'pa', 'pac', 'pace', 'pachinko', 'pack', 'pact', 'padlock', 'pagan', 'page', 'paid', 'pain', 'paint', 'paintbal', 'pair', 'pajama', 'pal', 'pale', 'palm', 'pamela', 'pan', 'pancak', 'pane', 'panic', 'pant', 'panti', 'paper', 'paperboy', 'parachut', 'paradis', 'paradox', 'paralyz', 'paranoia', 'paranoid', 'parasit', 'parent', 'pari', 'park', 'parlor', 'part', 'parti', 'particip', 'partner', 'pass', 'passeng', 'passion', 'past', 'path', 'pathet', 'patienc', 'patient', 'patrol', 'patron', 'pattern', 'paus', 'pave', 'pavement', 'pay', 'payback', 'payin', 'pe', 'peac', 'peak', 'pearl', 'peat', 'pedro', 'peek', 'peep', 'pen', 'pencil', 'pendulum', 'penicillin', 'penni', 'peopl', 'percent', 'percept', 'peregrin', 'perfect', 'perfum', 'perhap', 'peridis', 'perish', 'perk', 'perman', 'person', 'perspect', 'peru', 'pervert', 'pet', 'petal', 'pete', 'phantom', 'pharaoh', 'phifer', 'philant', 'philli', 'philosophi', 'phoenix', 'phone', 'phoney', 'photo', 'photograph', 'physic', 'pick', 'picket', 'pickup', 'pictur', 'pie', 'piec', 'pier', 'pig', 'pile', 'pill', 'pillow', 'pima', 'pimp', 'pinch', 'pink', 'pinnin', 'pinocchio', 'pinto', 'piper', 'pirat', 'piss', 'pit', 'pitch', 'piti', 'pixi', 'place', 'placin', 'plagu', 'plan', 'plane', 'planet', 'plaster', 'plastic', 'plate', 'platinum', 'platnum', 'platshirt', 'play', 'player', 'playground', 'playin', 'plea', 'plead', 'pleas', 'pleasent', 'pleasur', 'plenti', 'plete', 'plight', 'plot', 'plottin', 'plu', 'plug', 'plum', 'pocket', 'poet', 'point', 'pointi', 'pointin', 'pointless', 'poison', 'polic', 'polish', 'polit', 'pond', 'ponder', 'poni', 'ponytail', 'poof', 'pool', 'poor', 'pop', 'poppa', 'poppin', 'popstar', 'popul', 'popular', 'porch', 'port', 'portrait', 'pose', 'posess', 'posit', 'possibl', 'post', 'pot', 'potenc', 'potna', 'pound', 'pour', 'pow', 'power', 'powerhous', 'powter', 'practic', 'prais', 'prake', 'pray', 'prayer', 'prayin', 'preach', 'preacher', 'preachman', 'preciou', 'prefect', 'prefer', 'prelud', 'prepar', 'prerog', 'prescript', 'presid', 'press', 'pressur', 'pretend', 'pretens', 'pretti', 'prettier', 'prettiest', 'previou', 'pri', 'price', 'priceless', 'pride', 'priest', 'prima', 'primal', 'primit', 'princap', 'princip', 'print', 'prison', 'privaci', 'privat', 'prize', 'probabali', 'probabl', 'problem', 'procreat', 'produc', 'product', 'program', 'project', 'prolong', 'promis', 'proof', 'prop', 'propaganda', 'proper', 'prophet', 'proport', 'prosper', 'prostitut', 'protect', 'protest', 'protestor', 'proud', 'prove', 'provid', 'public', 'pucker', 'puff', 'puffi', 'pull', 'pullin', 'pulver', 'pump', 'punish', 'punk', 'pupil', 'puppet', 'puppi', 'purchas', 'pure', 'purpl', 'purpos', 'purs', 'push', 'pussi', 'put', 'puttin', 'qq', 'quadrupl', 'quarter', 'quarterback', 'quatro', 'queen', 'queer', 'quench', 'quenya', 'question', 'quick', 'quicker', 'quicki', 'quickli', 'quicksand', 'quiet', 'quietli', 'quit', 'quitt', 'quitter', 'quot', 'r', 'rabbit', 'race', 'racer', 'racetrack', 'racin', 'radio', 'rag', 'rage', 'railroad', 'railway', 'rain', 'rainbow', 'raini', 'rainth', 'rais', 'rake', 'ram', 'ran', 'rang', 'rap', 'rape', 'rapist', 'rappa', 'rapper', 'rappin', 'raptur', 'rase', 'rate', 'rave', 'ravish', 'raw', 'ray', 'razor', 'reach', 'react', 'reaction', 'read', 'readi', 'real', 'realis', 'realiti', 'realiz', 'realli', 'rearrang', 'reason', 'reassur', 'reawak', 'rebelli', 'rebellion', 'rebuild', 'recal', 'receptionist', 'reciev', 'reckless', 'reckon', 'reclus', 'recogn', 'recognis', 'recognit', 'record', 'recreat', 'rectifi', 'red', 'redneck', 'redpuls', 'reed', 'reel', 'reev', 'refin', 'reflect', 'refrain', 'refriger', 'refus', 'reggi', 'regiment', 'regist', 'regret', 'regular', 'reign', 'reject', 'rel', 'relaps', 'relationship', 'relax', 'releas', 'reli', 'reliev', 'religi', 'religion', 'reliv', 'relli', 'reload', 'remain', 'rememb', 'remi', 'remind', 'reminisc', 'remov', 'rent', 'rep', 'repair', 'repay', 'repeat', 'repent', 'replac', 'replacin', 'repli', 'report', 'repres', 'rerun', 'rescu', 'reserv', 'resid', 'resign', 'resist', 'resound', 'respect', 'respond', 'respons', 'ressurect', 'rest', 'restless', 'restrain', 'result', 'resurrect', 'retard', 'return', 'reveal', 'revel', 'reveng', 'reveri', 'revers', 'revolv', 'rewrit', 'rhe', 'rhyme', 'rhymin', 'rhythm', 'ribbon', 'rich', 'richochetin', 'ricki', 'rid', 'riddl', 'ride', 'ridicul', 'ridin', 'rig', 'righteou', 'ring', 'ringer', 'rip', 'ripper', 'rippin', 'rise', 'risk', 'rita', 'ritual', 'river', 'roach', 'road', 'roam', 'roar', 'rob', 'rock', 'rocket', 'roll', 'rollercoast', 'rollin', 'romanc', 'romant', 'rome', 'romeo', 'ronni', 'roof', 'room', 'roomat', 'root', 'rope', 'rose', 'rot', 'rotten', 'rottweil', 'rough', 'round', 'row', 'royal', 'rub', 'rubber', 'rud', 'rude', 'rug', 'ruin', 'rule', 'rumbl', 'rumin', 'rummag', 'rumor', 'run', 'runnin', 'rush', 'rushin', 'rust', 'rythym', 's', 'sack', 'sacr', 'sacrif', 'sacrific', 'sad', 'saddest', 'safe', 'safer', 'safeti', 'sail', 'saint', 'sake', 'salari', 'sale', 'salem', 'salt', 'salut', 'samba', 'san', 'sand', 'sanderest', 'sandman', 'sandpit', 'sang', 'santa', 'sareteta', 'sasaget', 'sat', 'satan', 'sate', 'satellit', 'satin', 'satisfact', 'satisfi', 'saturday', 'sauc', 'save', 'savin', 'savior', 'saww', 'sayin', 'scab', 'scam', 'scar', 'scare', 'scarf', 'scari', 'scene', 'scenest', 'scent', 'schedul', 'schmoe', 'school', 'sci', 'scissor', 'scooch', 'score', 'scrape', 'scratch', 'scream', 'screamin', 'screen', 'screw', 'scrotum', 'scrutin', 'scum', 'sea', 'seal', 'seam', 'sear', 'search', 'searchin', 'seasid', 'season', 'seat', 'secreci', 'secret', 'section', 'seed', 'seek', 'seeker', 'seep', 'seeth', 'segreg', 'sei', 'seiz', 'select', 'selfish', 'selfishli', 'sell', 'sellin', 'semi', 'send', 'sendin', 'sens', 'sensat', 'sensit', 'sentenc', 'sentient', 'separ', 'septemb', 'serenad', 'serial', 'seriou', 'serv', 'set', 'settl', 'seventi', 'sever', 'sew', 'sex', 'sexi', 'sh', 'shade', 'shadi', 'shadow', 'shadowi', 'shake', 'shaken', 'shallow', 'shame', 'shameless', 'shape', 'share', 'sharp', 'sharpen', 'shatter', 'shave', 'shavin', 'shawti', 'shed', 'sheep', 'sheet', 'shelf', 'shell', 'shelter', 'shi', 'shield', 'shift', 'shimmer', 'shine', 'shingl', 'shini', 'ship', 'shirt', 'shit', 'shiteru', 'shiver', 'shizzel', 'sho', 'shock', 'shoe', 'shoelac', 'shone', 'shook', 'shoot', 'shootin', 'shop', 'shore', 'short', 'shorti', 'shot', 'shotgun', 'shoulda', 'shoulder', 'shouldn', 'shout', 'shove', 'shovel', 'show', 'showbiz', 'showdown', 'shower', 'showin', 'shown', 'shrink', 'shroud', 'shrunk', 'shudder', 'shuffl', 'shut', 'shuuki', 'sick', 'sickl', 'side', 'sidewalk', 'sieg', 'siesta', 'sigh', 'sight', 'sign', 'signal', 'signific', 'silenc', 'silent', 'silli', 'silver', 'simpl', 'simpli', 'simplic', 'sin', 'sinc', 'sincer', 'sinew', 'sing', 'singer', 'singl', 'sink', 'sip', 'sippin', 'sir', 'siren', 'sissi', 'sister', 'sit', 'sitin', 'sittin', 'situat', 'sixteen', 'size', 'skate', 'skech', 'skeleton', 'ski', 'skid', 'skill', 'skin', 'skinni', 'skip', 'skirt', 'skit', 'skull', 'sky', 'slam', 'slap', 'slapper', 'slate', 'slave', 'slay', 'sled', 'sleep', 'sleeper', 'sleepi', 'sleepin', 'sleepless', 'slept', 'sli', 'slick', 'slide', 'slight', 'slightli', 'slim', 'slip', 'slipknot', 'slipper', 'slizzel', 'slope', 'slow', 'slower', 'slowin', 'slowli', 'slug', 'sluggin', 'slum', 'slut', 'smack', 'small', 'smaller', 'smart', 'smarter', 'smartest', 'smash', 'smear', 'smell', 'smelli', 'smile', 'smirk', 'smoke', 'smoki', 'smokin', 'smolder', 'smooth', 'smother', 'snail', 'snap', 'snappin', 'sniff', 'snooz', 'snortin', 'snow', 'snowfal', 'snuck', 'soak', 'soap', 'soar', 'sober', 'socialit', 'societi', 'sockhop', 'sodapop', 'sodom', 'sofa', 'soft', 'softer', 'softli', 'soggi', 'solac', 'sold', 'soldier', 'solemnli', 'soley', 'solo', 'solut', 'solv', 'somebodi', 'someday', 'someon', 'someth', 'sometim', 'somewher', 'somth', 'son', 'song', 'sono', 'sooner', 'sooo', 'sore', 'sorri', 'sorrow', 'sorrowen', 'sort', 'sou', 'soul', 'sound', 'soup', 'sour', 'sourc', 'south', 'souvenir', 'space', 'spade', 'spaghetti', 'spain', 'span', 'spanish', 'spare', 'spark', 'sparkl', 'sparklin', 'sparrow', 'spawn', 'speak', 'speaker', 'speakin', 'speci', 'special', 'speech', 'speed', 'spell', 'spend', 'spent', 'spice', 'spider', 'spiderman', 'spike', 'spill', 'spillin', 'spin', 'spine', 'spineless', 'spirit', 'spit', 'spite', 'splatter', 'split', 'spoke', 'spoken', 'spongi', 'spoon', 'sporad', 'sport', 'spose', 'spot', 'sprawl', 'spray', 'spread', 'spring', 'sprinkl', 'sprung', 'spun', 'squash', 'squeal', 'squint', 'sreamlin', 'sss', 'stab', 'stabbin', 'stabl', 'stack', 'stadio', 'stage', 'stain', 'stainless', 'stair', 'stake', 'stamina', 'stamp', 'stand', 'standin', 'star', 'stare', 'starin', 'starlight', 'start', 'startin', 'state', 'static', 'station', 'statu', 'stay', 'stayin', 'steadi', 'steal', 'stealin', 'stealth', 'steam', 'steed', 'steel', 'steepen', 'step', 'stepwritin', 'stereo', 'stick', 'stickin', 'stickler', 'sting', 'stinger', 'stingi', 'stink', 'stinki', 'stir', 'stock', 'stoke', 'stole', 'stolen', 'stomach', 'stone', 'stood', 'stop', 'store', 'stori', 'storm', 'stormi', 'stormiest', 'straight', 'strain', 'strand', 'strang', 'stranger', 'strangl', 'strap', 'straw', 'strawberri', 'stream', 'street', 'strength', 'stress', 'stressin', 'stretch', 'stricken', 'strictli', 'stride', 'strike', 'string', 'strip', 'stripe', 'stroll', 'strom', 'strong', 'stronger', 'strongest', 'struggl', 'strung', 'strut', 'stubborn', 'stuck', 'student', 'stuf', 'stuff', 'stuffi', 'stumbl', 'stunna', 'stupid', 'style', 'subject', 'sublim', 'sublimin', 'submarin', 'submerg', 'subtl', 'suburban', 'subway', 'suc', 'succeed', 'success', 'suck', 'sucka', 'sucker', 'sudden', 'suddenli', 'sue', 'suffer', 'suffic', 'suffici', 'sugar', 'suggest', 'suicid', 'suin', 'suit', 'suitcas', 'sukh', 'sulk', 'summer', 'summertim', 'sun', 'sunburn', 'sunday', 'sunflow', 'sung', 'sunk', 'sunni', 'sunris', 'sunset', 'sunshin', 'super', 'superfli', 'superman', 'supernatur', 'superson', 'superstardom', 'supper', 'support', 'suppos', 'suprem', 'sureteku', 'surf', 'surfac', 'surpris', 'surrend', 'surround', 'surviv', 'survivor', 'susi', 'suspect', 'suss', 'suteki', 'swallow', 'swam', 'swap', 'swarm', 'sway', 'swear', 'sweat', 'sweater', 'sweati', 'sweep', 'sweeper', 'sweet', 'sweeten', 'sweeter', 'sweetest', 'swellin', 'swept', 'swift', 'swim', 'swimmin', 'swindl', 'swindler', 'swine', 'swing', 'swirl', 'switch', 'sword', 'swore', 'symphoni', 'sync', 'syndrom', 'synthet', 'system', 't', 'tabatha', 'tabi', 'tabl', 'tachi', 'tackl', 'taco', 'taht', 'taker', 'takin', 'taklin', 'tale', 'talent', 'talk', 'talker', 'talkin', 'tall', 'taller', 'tan', 'tangerin', 'tango', 'tape', 'tar', 'target', 'tast', 'tastin', 'tattoo', 'taught', 'te', 'tea', 'teach', 'teacher', 'team', 'tear', 'teardrop', 'teas', 'tee', 'teenag', 'teeth', 'telephon', 'televis', 'teller', 'telli', 'tellin', 'temper', 'templ', 'tempo', 'tempt', 'temptat', 'ten', 'tenaci', 'tend', 'tender', 'tenni', 'tension', 'tent', 'tequila', 'terashidasu', 'term', 'termin', 'terribl', 'terrier', 'tesco', 'test', 'testifi', 'tether', 'text', 'tf', 'thang', 'thanksgiv', 'thaw', 'theatr', 'thee', 'thees', 'themselv', 'thereof', 'thesi', 'theyll', 'theyr', 'thhbbpp', 'thi', 'thick', 'thicker', 'thigh', 'thin', 'thing', 'thinkin', 'thinli', 'thirst', 'thirsti', 'thirteen', 'thirti', 'thorn', 'thought', 'thousand', 'thread', 'threat', 'threw', 'thrill', 'throat', 'throne', 'throuh', 'throw', 'throwin', 'thrown', 'thug', 'thumb', 'thunder', 'thursday', 'tian', 'tick', 'ticket', 'tickl', 'tide', 'tie', 'tiger', 'tight', 'tighten', 'tightli', 'til', 'till', 'tilt', 'time', 'tingli', 'tini', 'tiniest', 'tint', 'tion', 'tip', 'tipto', 'tire', 'tise', 'tissu', 'tivo', 'toad', 'toast', 'today', 'toe', 'togeth', 'told', 'toll', 'tomb', 'tomorrow', 'ton', 'tone', 'tongu', 'tonight', 'tooken', 'tooku', 'toon', 'tooth', 'top', 'toppin', 'torch', 'tore', 'torment', 'torn', 'toss', 'total', 'touch', 'touchi', 'tough', 'toumei', 'toupe', 'tour', 'tower', 'town', 'toy', 'trace', 'track', 'trade', 'tragedi', 'tragic', 'trail', 'train', 'trainer', 'traips', 'trait', 'tramp', 'tran', 'translat', 'transpar', 'trap', 'trappin', 'trash', 'trav', 'travel', 'travelin', 'travesti', 'tre', 'treasur', 'treat', 'tree', 'treehous', 'treelin', 'trembl', 'trend', 'tri', 'trial', 'tribe', 'trick', 'trigger', 'trip', 'tripl', 'triumphant', 'trophi', 'tropic', 'troubl', 'truck', 'true', 'truli', 'trunk', 'trust', 'truth', 'tryin', 'tryna', 'trynna', 'tsugi', 'tsuki', 'tsuzuiteku', 'tub', 'tube', 'tuck', 'tucker', 'tug', 'tune', 'tunnel', 'tupac', 'turkey', 'turn', 'tv', 'tweec', 'twelv', 'twenti', 'twiddl', 'twilight', 'twin', 'twinkl', 'twirl', 'twist', 'ty', 'type', 'u', 'ugli', 'uh', 'uhh', 'umi', 'ummm', 'unbeliev', 'unborn', 'unbroken', 'unbuckl', 'unchain', 'unclear', 'uncross', 'underground', 'underland', 'underneath', 'understand', 'understood', 'underwat', 'undon', 'undoubtedli', 'undress', 'unexpectedli', 'unfair', 'unfocus', 'unfold', 'unforgiv', 'unfreez', 'unfurl', 'unhappi', 'uniform', 'unit', 'uniti', 'univers', 'unkind', 'unknown', 'unlock', 'unlov', 'unmei', 'uno', 'unorigin', 'unplug', 'unpredict', 'unravel', 'unreason', 'unsaid', 'unseen', 'unselfishli', 'unsolv', 'unti', 'untrim', 'untru', 'upset', 'upsid', 'upstair', 'upstat', 'upstream', 'uptight', 'ur', 'urg', 'ust', 'usual', 'ut', 'utsuri', 'utter', 'v', 'vacat', 'vacuum', 'vagabond', 'vain', 'valedictorian', 'valentin', 'valentino', 'valley', 'vane', 'vanilla', 'vaniti', 'vari', 'vase', 've', 'veil', 'vein', 'velvet', 'venom', 'veri', 'verifi', 'vers', 'versac', 'version', 'vessel', 'vest', 'vfind', 'vibe', 'vic', 'vicadin', 'victim', 'victori', 'vietnam', 'view', 'vile', 'villain', 'villiana', 'vincent', 'vine', 'vinegar', 'violenc', 'violent', 'virdict', 'virgin', 'virtu', 'viru', 'vise', 'vision', 'visit', 'vival', 'vocabulari', 'vocal', 'vogu', 'voic', 'volum', 'volunt', 'vomit', 'vortex', 'vrin', 'vuitton', 'w', 'wa', 'wack', 'wag', 'wage', 'wail', 'waist', 'wait', 'waitin', 'wake', 'waken', 'walk', 'walkin', 'wall', 'wallac', 'wallet', 'wallow', 'walter', 'wander', 'wanna', 'wannab', 'wantin', 'war', 'ward', 'wari', 'warm', 'warmer', 'warmth', 'warn', 'warrior', 'wash', 'wasn', 'wast', 'watashi', 'watch', 'watchin', 'water', 'waterfal', 'watersh', 'wave', 'wavin', 'weak', 'weaken', 'weaker', 'weapon', 'wear', 'weari', 'weather', 'weatherman', 'weav', 'web', 'wed', 'weed', 'week', 'weekend', 'weep', 'weight', 'weird', 'welcom', 'wer', 'weren', 'west', 'wet', 'whale', 'whatcha', 'whatev', 'whaz', 'wheel', 'wheelchair', 'whenev', 'wherev', 'whi', 'whichev', 'whip', 'whippin', 'whirl', 'whiskey', 'whisper', 'whistl', 'whistler', 'white', 'whitney', 'whoa', 'whoo', 'whoop', 'whoopin', 'whoppin', 'whore', 'whylin', 'wick', 'wide', 'wife', 'wiith', 'wild', 'willin', 'win', 'wind', 'window', 'wine', 'wing', 'wink', 'winner', 'winter', 'wipe', 'wire', 'wisdom', 'wise', 'wishin', 'wist', 'wit', 'witcheri', 'withdrawl', 'wither', 'witnessin', 'wive', 'wizzel', 'wo', 'woah', 'woe', 'woh', 'woke', 'woken', 'wolf', 'woman', 'womb', 'women', 'won', 'wond', 'wont', 'woo', 'wood', 'wooden', 'woohohoo', 'woop', 'word', 'wore', 'work', 'workday', 'workin', 'world', 'worn', 'worri', 'wors', 'worship', 'worshippin', 'worst', 'worth', 'worthi', 'worthless', 'worthwhil', 'woulda', 'wouldn', 'wouldnt', 'wound', 'wouuld', 'wrack', 'wrap', 'wreck', 'wreckag', 'wrestl', 'wrestler', 'wristwatch', 'write', 'writer', 'writin', 'written', 'wrong', 'wrote', 'wussup', 'xl', 'xma', 'xn', 'y', 'ya', 'yacht', 'yaeah', 'yaeh', 'yappin', 'yard', 'yawn', 'ye', 'yea', 'yeah', 'yeahhh', 'year', 'yearn', 'yeeaah', 'yeh', 'yell', 'yellow', 'yer', 'yesterday', 'yo', 'yor', 'york', 'youll', 'young', 'youngster', 'youth', 'yoyo', 'ysl', 'yuku', 'yum', 'yup', 'yyt', 'z', 'zambuca', 'zapatista', 'zieg', 'zion', 'zombi', 'zone', 'zoo', 'zoom', 'zwieback']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "OaydPMx8Ga3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "\n",
        "# `pos_label` for positive class, since we have sad=1, happy=0\n",
        "\n",
        "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, average = 'macro')"
      ],
      "metadata": {
        "id": "fzEJYGPAGjrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "pipeline_1 = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('clf', BernoulliNB())\n",
        "])\n",
        "\n",
        "parameters_1 = dict(\n",
        "    vect__binary=[True],\n",
        "    vect__stop_words=[stop_words, None],\n",
        "    vect__tokenizer=[porter_tokenizer, None],\n",
        "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
        ")\n",
        "\n",
        "grid_search_1 = GridSearchCV(pipeline_1, \n",
        "                           parameters_1, \n",
        "                           n_jobs=1, \n",
        "                           verbose=1,\n",
        "                           scoring=f1_scorer,\n",
        "                           cv=10\n",
        "                )\n",
        "\n",
        "\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in pipeline_1.steps])\n",
        "print(\"parameters:\")\n",
        "pprint(parameters_1, depth=2)\n",
        "grid_search_1.fit(X_train, y_train)\n",
        "print(\"Best score: %0.3f\" % grid_search_1.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters_1 = grid_search_1.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters_1.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIRmRnm3MnQJ",
        "outputId": "e39f820e-dedf-4935-f063-f06cec8e328a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['vect', 'clf']\n",
            "parameters:\n",
            "{'vect__binary': [True],\n",
            " 'vect__ngram_range': [(...), (...), (...)],\n",
            " 'vect__stop_words': [[...], None],\n",
            " 'vect__tokenizer': [<function porter_tokenizer at 0x7f446d347710>, None]}\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score: 0.339\n",
            "Best parameters set:\n",
            "\tvect__binary: True\n",
            "\tvect__ngram_range: (1, 1)\n",
            "\tvect__stop_words: None\n",
            "\tvect__tokenizer: <function porter_tokenizer at 0x7f446d347710>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_3 = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "parameters_3 = dict(\n",
        "    vect__binary=[False],\n",
        "    vect__stop_words=[stop_words, None],\n",
        "    vect__tokenizer=[porter_tokenizer, None],\n",
        "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
        ")\n",
        "\n",
        "grid_search_3 = GridSearchCV(pipeline_3, \n",
        "                           parameters_3, \n",
        "                           n_jobs=1, \n",
        "                           verbose=1,\n",
        "                           scoring=f1_scorer,\n",
        "                           cv=10\n",
        "                )\n",
        "\n",
        "\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in pipeline_3.steps])\n",
        "print(\"parameters:\")\n",
        "pprint(parameters_3, depth=2)\n",
        "grid_search_3.fit(X_train, y_train)\n",
        "print(\"Best score: %0.3f\" % grid_search_3.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters_3 = grid_search_3.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters_3.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters_3[param_name]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlzojiNTNFTs",
        "outputId": "7222071e-2b3a-4247-fd50-41af537712dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['vect', 'clf']\n",
            "parameters:\n",
            "{'vect__binary': [False],\n",
            " 'vect__ngram_range': [(...), (...), (...)],\n",
            " 'vect__stop_words': [[...], None],\n",
            " 'vect__tokenizer': [<function porter_tokenizer at 0x7f446d347710>, None]}\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score: 0.341\n",
            "Best parameters set:\n",
            "\tvect__binary: False\n",
            "\tvect__ngram_range: (1, 1)\n",
            "\tvect__stop_words: None\n",
            "\tvect__tokenizer: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pipeline_4 = Pipeline([\n",
        "    ('vect', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "parameters_4 = dict(\n",
        "    vect__binary=[False],\n",
        "    vect__stop_words=[stop_words, None],\n",
        "    vect__tokenizer=[porter_tokenizer, None],\n",
        "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
        ")\n",
        "\n",
        "grid_search_4 = GridSearchCV(pipeline_4, \n",
        "                           parameters_4, \n",
        "                           n_jobs=1, \n",
        "                           verbose=1,\n",
        "                           scoring=f1_scorer,\n",
        "                           cv=10\n",
        "                )\n",
        "\n",
        "\n",
        "print(\"Performing grid search...\")\n",
        "print(\"pipeline:\", [name for name, _ in pipeline_4.steps])\n",
        "print(\"parameters:\")\n",
        "pprint(parameters_4, depth=2)\n",
        "grid_search_4.fit(X_train, y_train)\n",
        "print(\"Best score: %0.3f\" % grid_search_4.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters_4 = grid_search_4.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters_4.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters_4[param_name]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uk77tJpwPO8",
        "outputId": "91c618e3-07d2-49d7-c127-bc75be5cc0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing grid search...\n",
            "pipeline: ['vect', 'clf']\n",
            "parameters:\n",
            "{'vect__binary': [False],\n",
            " 'vect__ngram_range': [(...), (...), (...)],\n",
            " 'vect__stop_words': [[...], None],\n",
            " 'vect__tokenizer': [<function porter_tokenizer at 0x7f446d347710>, None]}\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['a', 'abl', 'abov', 'accord', 'accordingli', 'actual', 'afterward', 'ain', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anybodi', 'anyon', 'anyth', 'anywher', 'appreci', 'appropri', 'aren', 'asid', 'associ', 'avail', 'aw', 'becam', 'becaus', 'becom', 'befor', 'believ', 'besid', 'c', 'caus', 'certainli', 'chang', 'clearli', 'concern', 'consequ', 'consid', 'correspond', 'couldn', 'cours', 'current', 'd', 'definit', 'describ', 'despit', 'didn', 'differ', 'doe', 'doesn', 'don', 'downward', 'dure', 'els', 'elsewher', 'entir', 'especi', 'everi', 'everybodi', 'everyon', 'everyth', 'everywher', 'exactli', 'exampl', 'follow', 'formerli', 'furthermor', 'give', 'goe', 'greet', 'ha', 'hadn', 'happen', 'hardli', 'hasn', 'haven', 'henc', 'hereaft', 'herebi', 'hope', 'howev', 'i', 'ignor', 'immedi', 'inde', 'indic', 'isn', 'late', 'latterli', 'littl', 'll', 'm', 'mainli', 'mani', 'mayb', 'meanwhil', 'mere', 'mon', 'moreov', 'mostli', 'nearli', 'necessari', 'nobodi', 'noon', 'normal', 'noth', 'nowher', 'obvious', 'onc', 'onli', 'otherwis', 'ourselv', 'outsid', 'overal', 'particularli', 'perhap', 'place', 'pleas', 'plu', 'possibl', 'presum', 'probabl', 'provid', 'quit', 'realli', 'reason', 'regard', 'rel', 'respect', 's', 'secondli', 'selv', 'sensibl', 'seriou', 'sever', 'shouldn', 'sinc', 'somebodi', 'someon', 'someth', 'sometim', 'somewher', 'sorri', 'specifi', 't', 'tend', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thoroughli', 'thu', 'togeth', 'tri', 'truli', 'unfortun', 'unlik', 'usual', 'valu', 'variou', 've', 'veri', 'wa', 'wasn', 'welcom', 'weren', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'won', 'wouldn', 'ye', 'yourselv'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score: 0.334\n",
            "Best parameters set:\n",
            "\tvect__binary: False\n",
            "\tvect__ngram_range: (1, 1)\n",
            "\tvect__stop_words: None\n",
            "\tvect__tokenizer: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy import interp\n",
        "\n",
        "sns.set()\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "clf_1 = Pipeline([\n",
        "                  ('vect', CountVectorizer(\n",
        "                                           binary=True,\n",
        "                                           stop_words=stop_words,\n",
        "                                           tokenizer=porter_tokenizer,\n",
        "                                           ngram_range=(1,1),\n",
        "                                           )\n",
        "                 ),\n",
        "                 ('clf', BernoulliNB()),\n",
        "                 ])\n",
        "\n",
        "clf_2 = Pipeline([\n",
        "                  ('vect', CountVectorizer(\n",
        "                                           binary=False,\n",
        "                                           stop_words=stop_words,\n",
        "                                           tokenizer=porter_tokenizer,\n",
        "                                           ngram_range=(1,1),\n",
        "                                           )\n",
        "                 ),\n",
        "                 ('clf', MultinomialNB()),\n",
        "                 ])\n",
        "\n",
        "clf_3 = Pipeline([\n",
        "                  ('vect', TfidfVectorizer(\n",
        "                                           binary=False,\n",
        "                                           stop_words=stop_words,\n",
        "                                           tokenizer=porter_tokenizer,\n",
        "                                           ngram_range=(1,1),\n",
        "                                           )\n",
        "                 ),\n",
        "                 ('clf', MultinomialNB()),\n",
        "                 ])\n",
        "\n",
        "colors = ['#1947D1', '#CC3300', 'k']\n",
        "linestyles = ['-', '--', '-.']\n",
        "classifiers = [clf_1, clf_2, clf_3]\n",
        "labels = ['1: MV Bernoulli NB, stop words, porter stemmer, \\nuni-gram, df', \n",
        "          '2: Multinomial NB, stop words, porter stemmer, \\nuni-gram, tf',\n",
        "          '3: Multinomial NB, stop words, porter stemmer, \\nuni-gram, tf-idf',\n",
        "          ]\n",
        "\n",
        "for clf,col,ls,lab in zip(classifiers, colors, linestyles, labels):\n",
        "    \n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    all_tpr = []\n",
        "    cv = StratifiedKFold(n_splits = 10)\n",
        "\n",
        "    for i, (train, test) in enumerate(cv.get_n_splits(y_train)):\n",
        "        probas_ = clf.fit(X_train[train], y_train[train]).predict_proba(X_train[test])\n",
        "        # Compute ROC curve and area the curve\n",
        "        fpr, tpr, thresholds = roc_curve(y_train[test], probas_[:, 1])\n",
        "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "        mean_tpr[0] = 0.0\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    mean_tpr /= len(cv)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    plt.plot(mean_fpr, \n",
        "             mean_tpr, \n",
        "             color=col, \n",
        "             linestyle=ls,\n",
        "             label='%s (ROC AUC = %0.2f)' % (lab, mean_auc), \n",
        "             lw=2\n",
        "    )\n",
        "\n",
        "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random Guessing')    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('./images/roc_gridsearch_1.eps', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "Ml5oBh5kxBMH",
        "outputId": "77b573ec-967b-46ea-de61-9d1fab0d1be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-26ee4f45d9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprobas_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Compute ROC curve and area the curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    }
  ]
}