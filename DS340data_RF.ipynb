{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonZQH/DS340W/blob/main/DS340data_RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Mlxtend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUtdAFScZY_r",
        "outputId": "980d297c-4d18-4017-f008-76a4961644dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Mlxtend in /usr/local/lib/python3.8/dist-packages (0.14.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.17.1->Mlxtend) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->Mlxtend) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->Mlxtend) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->Mlxtend) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0nSiCcC6Sgvw",
        "outputId": "f42fefb5-b9de-4b90-8afc-aece76eaa43c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0       Time                 data label\n",
              "20110          48  [03:36.79     It is meaningful   Sad\n",
              "20111          49  [03:41.90      It is wonderful   Sad\n",
              "20112          50  [03:46.64     It is meaningful   Sad\n",
              "20113          51  [03:51.28  It goes full circle   Sad\n",
              "20114          52  [03:58.30                  NaN   Sad"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0905f704-86f3-4114-b2ff-79bc863f2408\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Time</th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20110</th>\n",
              "      <td>48</td>\n",
              "      <td>[03:36.79</td>\n",
              "      <td>It is meaningful</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20111</th>\n",
              "      <td>49</td>\n",
              "      <td>[03:41.90</td>\n",
              "      <td>It is wonderful</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20112</th>\n",
              "      <td>50</td>\n",
              "      <td>[03:46.64</td>\n",
              "      <td>It is meaningful</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20113</th>\n",
              "      <td>51</td>\n",
              "      <td>[03:51.28</td>\n",
              "      <td>It goes full circle</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20114</th>\n",
              "      <td>52</td>\n",
              "      <td>[03:58.30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0905f704-86f3-4114-b2ff-79bc863f2408')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0905f704-86f3-4114-b2ff-79bc863f2408 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0905f704-86f3-4114-b2ff-79bc863f2408');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "df_train.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.dropna(how = 'any')\n",
        "df_test = df_test.dropna(how = 'any')\n",
        "print(df_train.tail(100))\n",
        "print(df_test.tail(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRz_qTG2bk5k",
        "outputId": "67a61354-0f95-46aa-c1cf-4966ed3e6de2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Unnamed: 0       Time                                         data  \\\n",
            "19993          75  [02:30.76  You won't find me, the past is so behind me   \n",
            "19995          77  [02:37.64                           Buried in the snow   \n",
            "19996          78  [02:42.34                                            .   \n",
            "19998          80  [02:43.40                                    Let it go   \n",
            "19999          81  [02:44.90                                    Let it go   \n",
            "...           ...        ...                                          ...   \n",
            "20109          47  [03:31.95                           It is so wonderful   \n",
            "20110          48  [03:36.79                             It is meaningful   \n",
            "20111          49  [03:41.90                              It is wonderful   \n",
            "20112          50  [03:46.64                             It is meaningful   \n",
            "20113          51  [03:51.28                          It goes full circle   \n",
            "\n",
            "      label  \n",
            "19993   Sad  \n",
            "19995   Sad  \n",
            "19996   Sad  \n",
            "19998   Sad  \n",
            "19999   Sad  \n",
            "...     ...  \n",
            "20109   Sad  \n",
            "20110   Sad  \n",
            "20111   Sad  \n",
            "20112   Sad  \n",
            "20113   Sad  \n",
            "\n",
            "[100 rows x 4 columns]\n",
            "       Unnamed: 0       Time                                      data label\n",
            "18284          23  [02:03.49      From the cradles they were rocked in   Sad\n",
            "18285          24  [02:08.18  You took the first words that they spoke   Sad\n",
            "18286          25  [02:12.44                            Yeah you stole   Sad\n",
            "18287          26  [02:17.36                            Yeah you stole   Sad\n",
            "18288          27  [02:22.77       So if I'm a liar and you're a thief   Sad\n",
            "...           ...        ...                                       ...   ...\n",
            "18391          30  [03:02.07               In your head they are dying   Sad\n",
            "18393          32  [03:06.76                In your head, in your head   Sad\n",
            "18394          33  [03:12.01                    Zombie, zombie, zombie   Sad\n",
            "18395          34  [03:18.00         What's in your head, in your head   Sad\n",
            "18396          35  [03:23.52                    Zombie, zombie, zombie   Sad\n",
            "\n",
            "[100 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[~df_train['data'].str.startswith('[','.')]\n",
        "print(df_train)\n",
        "#df_test = df_test[~df_test['data'].str.startswith('[','.')]\n",
        "print(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHO2wlh0FMAw",
        "outputId": "e471bc6c-ff7d-4aaf-ea23-15325042fe03"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Unnamed: 0       Time                            data  label\n",
            "0               0  [00:12.55  Another day wasted out of time  Angry\n",
            "1               1  [00:14.69         I can't get out of this  Angry\n",
            "2               2  [00:16.03           Altered state of mind  Angry\n",
            "3               3  [00:17.41             I'm going overboard  Angry\n",
            "4               4  [00:18.75     My conscience meets decline  Angry\n",
            "...           ...        ...                             ...    ...\n",
            "20109          47  [03:31.95              It is so wonderful    Sad\n",
            "20110          48  [03:36.79                It is meaningful    Sad\n",
            "20111          49  [03:41.90                 It is wonderful    Sad\n",
            "20112          50  [03:46.64                It is meaningful    Sad\n",
            "20113          51  [03:51.28             It goes full circle    Sad\n",
            "\n",
            "[17622 rows x 4 columns]\n",
            "       Unnamed: 0       Time  \\\n",
            "0               0  [00:34.61   \n",
            "1               1  [00:39.10   \n",
            "2               2  [00:43.86   \n",
            "3               3  [00:47.26   \n",
            "4               4  [00:51.77   \n",
            "...           ...        ...   \n",
            "18391          30  [03:02.07   \n",
            "18393          32  [03:06.76   \n",
            "18394          33  [03:12.01   \n",
            "18395          34  [03:18.00   \n",
            "18396          35  [03:23.52   \n",
            "\n",
            "                                                    data  label  \n",
            "0               Sometimes you just feel tired, feel weak  Angry  \n",
            "1      When you feel weak, you feel like you just wan...  Angry  \n",
            "2      But you gotta search within you, try and find ...  Angry  \n",
            "3      And just pull that shit out of you, and get th...  Angry  \n",
            "4      And not be a quitter no matter how bad you wan...  Angry  \n",
            "...                                                  ...    ...  \n",
            "18391                        In your head they are dying    Sad  \n",
            "18393                         In your head, in your head    Sad  \n",
            "18394                             Zombie, zombie, zombie    Sad  \n",
            "18395                  What's in your head, in your head    Sad  \n",
            "18396                             Zombie, zombie, zombie    Sad  \n",
            "\n",
            "[16152 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train['data']\n",
        "y_train = df_train['label']\n",
        "\n",
        "X_test = df_test['data']\n",
        "y_test = df_test['label']"
      ],
      "metadata": {
        "id": "aMK2SockSy0f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "X_train = df_train['data'].values \n",
        "\n",
        "y_train = df_train['label'].values\n",
        "\n",
        "print('before: %s ...' %y_train[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "print('after: %s ...' %y_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgab3ExgTN6s",
        "outputId": "74c1e221-fd11-4140-d00f-0f490572de20"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: ['Angry' 'Angry' 'Angry' 'Angry' 'Angry'] ...\n",
            "after: [0 0 0 0 0] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "stop_words = pickle.load(open('./stopwords.p', 'rb'))\n",
        "semantic_words = pickle.load(open('./semantic_words_py34.p', 'rb'))"
      ],
      "metadata": {
        "id": "ptkx8zwAXu_1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = EnglishStemmer()\n",
        "\n",
        "# raw words\n",
        "tokenizer = lambda text: text.split()\n",
        "\n",
        "# words after Porter stemming \n",
        "tokenizer_porter = lambda text: [porter.stem(word) for word in text.split()]\n",
        "\n",
        "# Words after Snowball stemming\n",
        "tokenizer_snowball = lambda text: [snowball.stem(word) for word in text.split()]\n",
        "\n",
        "# Only words that are in a list of 'positive' or 'negative' words ('whitelist')\n",
        "# http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\n",
        "tokenizer_whitelist = lambda text: [word for word in text.split() if word in semantic_words]\n",
        "\n",
        "# Porter-stemmed words in whitelist\n",
        "tokenizer_porter_wl = lambda text: [porter.stem(word) for word in text.split() if word in semantic_words]\n",
        "\n",
        "# Snowball-stemmed words in whitelist\n",
        "tokenizer_snowball_wl = lambda text: [snowball.stem(word) for word in text.split() if word in semantic_words]"
      ],
      "metadata": {
        "id": "k_UjqgeXYKQf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "\n",
        "vect_1 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "vect_2 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter)\n",
        "    \n",
        "vect_3 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball)  \n",
        "\n",
        "vect_4 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_whitelist)  \n",
        "vect_5 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter_wl)\n",
        "\n",
        "vect_6 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball_wl)\n",
        "\n",
        "vect_7 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "vect_8 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter)\n",
        "    \n",
        "vect_9 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball)\n",
        "\n",
        "vect_10 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_whitelist)    \n",
        "vect_11 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter_wl)\n",
        "\n",
        "vect_12 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball_wl)\n",
        "\n",
        "\n",
        "pipelines = []\n",
        "vectorizers = [vect_1, vect_2, vect_3, vect_4, vect_5, vect_6, vect_7, vect_8, vect_9, vect_10, vect_11, vect_12]\n",
        "for v in vectorizers:\n",
        "    pipelines.append(Pipeline([('vect', v),\n",
        "                               ('dense', DenseTransformer()),\n",
        "                               ('clf', RandomForestClassifier(n_estimators=100))]))"
      ],
      "metadata": {
        "id": "pWE2jQPHYP9O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Vocabulary sizes\\n')\n",
        "labels = ['CountVec', 'CountVec porter', 'CountVec snowball', 'CountVec wl', 'CountVec porter+wl','CountVec snowball+wl',\n",
        "          'TfidfVec', 'TfidfVec porter', 'TfidfVec snowball', 'TfidfVec wl', 'TfidfVec porter+wl','TfidfVec snowball+wl',]\n",
        "X_train = df_train['data'].values.astype('U')\n",
        "#print(X_train)\n",
        "for label, v in zip(labels, vectorizers):\n",
        "    v.fit(X_train)\n",
        "    print('%s: %s' % (label, len(v.vocabulary_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fR4b5VjZvp5",
        "outputId": "5685fdff-3bee-468d-e26c-d337c96ef12d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary sizes\n",
            "\n",
            "CountVec: 5788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVec porter: 4474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVec snowball: 4457\n",
            "CountVec wl: 1013\n",
            "CountVec porter+wl: 831\n",
            "CountVec snowball+wl: 824\n",
            "TfidfVec: 5788\n",
            "TfidfVec porter: 4474\n",
            "TfidfVec snowball: 4457\n",
            "TfidfVec wl: 1013\n",
            "TfidfVec porter+wl: 831\n",
            "TfidfVec snowball+wl: 824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "labels = ['CountVec', 'CountVec porter', 'CountVec snowball', 'CountVec wl', 'CountVec porter+wl','CountVec snowball+wl',\n",
        "          'TfidfVec', 'TfidfVec porter', 'TfidfVec snowball', 'TfidfVec wl', 'TfidfVec porter+wl','TfidfVec snowball+wl',]\n",
        "\n",
        "\n",
        "\n",
        "d = {'Data':labels,\n",
        "     'Accuracy (%)':[],}\n",
        "\n",
        "for i,clf in enumerate(pipelines):\n",
        "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, scoring='accuracy', cv=10)\n",
        "    print('clf %s, %s: %s' % (i+1, labels[i], scores.mean()*100))\n",
        "    d['Accuracy (%)'].append('%0.2f (+/- %0.2f)' % (scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzaQxmuTG_lq",
        "outputId": "f6d9ef2f-6444-4156-ff7d-784c53129787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 1, CountVec: 32.090534849597894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 2, CountVec porter: 32.919080764072696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_perform = pd.DataFrame(d)\n",
        "df_perform = df_perform['Accuracy (%)']\n",
        "df_perform.index=(labels)\n",
        "df_perform"
      ],
      "metadata": {
        "id": "gBuQ1OUgdDoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect = TfidfVectorizer(binary=False,\n",
        "                       stop_words=stop_words,\n",
        "                       ngram_range=(1,1),\n",
        "                       preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                       tokenizer=lambda text: [porter.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "MIRg8UUe5Nuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "pipe_1 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=50))])\n",
        "\n",
        "pipe_2 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=100))])\n",
        "\n",
        "pipe_3 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=200))])\n",
        "\n",
        "pipe_4 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=400))])\n",
        "\n",
        "labels = [50, 100, 200, 400]\n",
        "\n",
        "for i,clf in enumerate([pipe_1, pipe_2, pipe_3, pipe_4]):\n",
        "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, scoring='accuracy', cv=10)\n",
        "    print('clf %s, %s: %0.2f (+/- %0.2f)' % (i+1, labels[i], scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "D-NTve-U5hf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_feat = vect.fit_transform(X_train, y_train)\n",
        "X_train_feat = X_train_feat.toarray()\n",
        "\n",
        "print(X_train_feat)"
      ],
      "metadata": {
        "id": "V2QHBS1F580c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "clf_3 = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "\n",
        "tuned_parameters = [\n",
        "  {'criterion': ['gini', 'entropy'], \n",
        "   'max_features': ['auto', 'log2', 'sqrt'],\n",
        "   'min_samples_split':[2,3], \n",
        "   'min_samples_leaf':[1,2]},\n",
        " ]\n",
        "\n",
        "\n",
        "grid_search_1 = GridSearchCV(clf_3, \n",
        "                           tuned_parameters, \n",
        "                           n_jobs=1, \n",
        "                           scoring='accuracy',\n",
        "                           cv=10\n",
        "                )\n",
        "\n",
        "grid_search_1.fit(X_train_feat, y_train)\n",
        "\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print()\n",
        "print(grid_search_1.best_estimator_)\n",
        "print()\n",
        "print(\"Grid scores on development set:\")\n",
        "print()\n",
        "for params, mean_score, scores in grid_search_1.grid_scores_:\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "            % (mean_score, scores.std() / 2, params))"
      ],
      "metadata": {
        "id": "A5nSJ0Ll6BA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# `pos_label` for positive class, since we have sad=1, happy=0\n",
        "\n",
        "acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n",
        "pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, pos_label=0)\n",
        "rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, pos_label=0)\n",
        "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=0)"
      ],
      "metadata": {
        "id": "qRATmB5t6K07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Train CountVec', 'Train CountVec porter', 'Train CountVec snowball', 'Train CountVec wl', \n",
        "          'Train CountVec porter+wl','Train CountVec snowball+wl',\n",
        "          'Train TfidfVec', 'Train TfidfVec porter', 'Train TfidfVec snowball', 'Train TfidfVec wl', \n",
        "          'Train TfidfVec porter+wl','Train TfidfVec snowball+wl',\n",
        "          'Test CountVec', 'Test CountVec porter', 'Test CountVec snowball', 'Test CountVec wl', \n",
        "          'Test CountVec porter+wl','Test CountVec snowball+wl',\n",
        "          'Test TfidfVec', 'Test TfidfVec porter', 'Test TfidfVec snowball', 'Test TfidfVec wl', \n",
        "          'Test TfidfVec porter+wl','Test TfidfVec snowball+wl',]\n",
        "\n",
        "d = {'Data':labels,\n",
        "     'ACC (%)':[],\n",
        "     'PRE (%)':[],\n",
        "     'REC (%)':[],\n",
        "     'F1 (%)':[],\n",
        "}\n",
        "\n",
        "for clf in pipelines:\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "for clf in pipelines:\n",
        "\n",
        "    d['ACC (%)'].append(acc_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "    d['PRE (%)'].append(pre_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "    d['REC (%)'].append(rec_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "    d['F1 (%)'].append(f1_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "\n",
        "for clf in pipelines:\n",
        "\n",
        "    d['ACC (%)'].append(acc_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
        "    d['PRE (%)'].append(pre_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
        "    d['REC (%)'].append(rec_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
        "    d['F1 (%)'].append(f1_scorer(estimator=clf, X=X_test, y_true=y_test))"
      ],
      "metadata": {
        "id": "chO4aycn6MKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('precision', 2)\n",
        "\n",
        "df_perform = pd.DataFrame(d)\n",
        "df_perform = df_perform[['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)']]\n",
        "df_perform.index=(labels)\n",
        "df_perform = df_perform*100\n",
        "df_perform = np.round(df_perform, decimals=2)\n",
        "df_perform"
      ],
      "metadata": {
        "id": "vXcqfUyM6hXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}