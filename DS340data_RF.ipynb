{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Mlxtend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUtdAFScZY_r",
        "outputId": "e6082fa8-3a22-4b5d-ed12-ec8ea477a5b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Mlxtend in /usr/local/lib/python3.8/dist-packages (0.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (57.4.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.7.3)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.8/dist-packages (from Mlxtend) (1.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.1->Mlxtend) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.17.1->Mlxtend) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->Mlxtend) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->Mlxtend) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->Mlxtend) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0nSiCcC6Sgvw",
        "outputId": "e86a3dd1-6d4b-4b87-e863-d65499c53b08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0       Time                 data label\n",
              "20110          48  [03:36.79     It is meaningful   Sad\n",
              "20111          49  [03:41.90      It is wonderful   Sad\n",
              "20112          50  [03:46.64     It is meaningful   Sad\n",
              "20113          51  [03:51.28  It goes full circle   Sad\n",
              "20114          52  [03:58.30                  NaN   Sad"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d53da8ea-04f0-4afe-bb8e-e2745102bfde\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Time</th>\n",
              "      <th>data</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20110</th>\n",
              "      <td>48</td>\n",
              "      <td>[03:36.79</td>\n",
              "      <td>It is meaningful</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20111</th>\n",
              "      <td>49</td>\n",
              "      <td>[03:41.90</td>\n",
              "      <td>It is wonderful</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20112</th>\n",
              "      <td>50</td>\n",
              "      <td>[03:46.64</td>\n",
              "      <td>It is meaningful</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20113</th>\n",
              "      <td>51</td>\n",
              "      <td>[03:51.28</td>\n",
              "      <td>It goes full circle</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20114</th>\n",
              "      <td>52</td>\n",
              "      <td>[03:58.30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d53da8ea-04f0-4afe-bb8e-e2745102bfde')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d53da8ea-04f0-4afe-bb8e-e2745102bfde button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d53da8ea-04f0-4afe-bb8e-e2745102bfde');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "df_train.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.dropna(how = 'any')\n",
        "df_test = df_test.dropna(how = 'any')\n",
        "print(df_train.tail(100))\n",
        "print(df_test.tail(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRz_qTG2bk5k",
        "outputId": "e5f5eb02-e477-41f7-f052-42f13e7397d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Unnamed: 0       Time                                         data  \\\n",
            "19993          75  [02:30.76  You won't find me, the past is so behind me   \n",
            "19995          77  [02:37.64                           Buried in the snow   \n",
            "19996          78  [02:42.34                                            .   \n",
            "19998          80  [02:43.40                                    Let it go   \n",
            "19999          81  [02:44.90                                    Let it go   \n",
            "...           ...        ...                                          ...   \n",
            "20109          47  [03:31.95                           It is so wonderful   \n",
            "20110          48  [03:36.79                             It is meaningful   \n",
            "20111          49  [03:41.90                              It is wonderful   \n",
            "20112          50  [03:46.64                             It is meaningful   \n",
            "20113          51  [03:51.28                          It goes full circle   \n",
            "\n",
            "      label  \n",
            "19993   Sad  \n",
            "19995   Sad  \n",
            "19996   Sad  \n",
            "19998   Sad  \n",
            "19999   Sad  \n",
            "...     ...  \n",
            "20109   Sad  \n",
            "20110   Sad  \n",
            "20111   Sad  \n",
            "20112   Sad  \n",
            "20113   Sad  \n",
            "\n",
            "[100 rows x 4 columns]\n",
            "       Unnamed: 0       Time                                      data label\n",
            "18284          23  [02:03.49      From the cradles they were rocked in   Sad\n",
            "18285          24  [02:08.18  You took the first words that they spoke   Sad\n",
            "18286          25  [02:12.44                            Yeah you stole   Sad\n",
            "18287          26  [02:17.36                            Yeah you stole   Sad\n",
            "18288          27  [02:22.77       So if I'm a liar and you're a thief   Sad\n",
            "...           ...        ...                                       ...   ...\n",
            "18391          30  [03:02.07               In your head they are dying   Sad\n",
            "18393          32  [03:06.76                In your head, in your head   Sad\n",
            "18394          33  [03:12.01                    Zombie, zombie, zombie   Sad\n",
            "18395          34  [03:18.00         What's in your head, in your head   Sad\n",
            "18396          35  [03:23.52                    Zombie, zombie, zombie   Sad\n",
            "\n",
            "[100 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[~df_train['data'].str.startswith('[','.')]\n",
        "print(df_train)\n",
        "#df_test = df_test[~df_test['data'].str.startswith('[','.')]\n",
        "print(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHO2wlh0FMAw",
        "outputId": "583b98f6-dfe8-4a53-b527-6e8371ce4ff2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Unnamed: 0       Time                            data  label\n",
            "0               0  [00:12.55  Another day wasted out of time  Angry\n",
            "1               1  [00:14.69         I can't get out of this  Angry\n",
            "2               2  [00:16.03           Altered state of mind  Angry\n",
            "3               3  [00:17.41             I'm going overboard  Angry\n",
            "4               4  [00:18.75     My conscience meets decline  Angry\n",
            "...           ...        ...                             ...    ...\n",
            "20109          47  [03:31.95              It is so wonderful    Sad\n",
            "20110          48  [03:36.79                It is meaningful    Sad\n",
            "20111          49  [03:41.90                 It is wonderful    Sad\n",
            "20112          50  [03:46.64                It is meaningful    Sad\n",
            "20113          51  [03:51.28             It goes full circle    Sad\n",
            "\n",
            "[17622 rows x 4 columns]\n",
            "       Unnamed: 0       Time  \\\n",
            "0               0  [00:34.61   \n",
            "1               1  [00:39.10   \n",
            "2               2  [00:43.86   \n",
            "3               3  [00:47.26   \n",
            "4               4  [00:51.77   \n",
            "...           ...        ...   \n",
            "18391          30  [03:02.07   \n",
            "18393          32  [03:06.76   \n",
            "18394          33  [03:12.01   \n",
            "18395          34  [03:18.00   \n",
            "18396          35  [03:23.52   \n",
            "\n",
            "                                                    data  label  \n",
            "0               Sometimes you just feel tired, feel weak  Angry  \n",
            "1      When you feel weak, you feel like you just wan...  Angry  \n",
            "2      But you gotta search within you, try and find ...  Angry  \n",
            "3      And just pull that shit out of you, and get th...  Angry  \n",
            "4      And not be a quitter no matter how bad you wan...  Angry  \n",
            "...                                                  ...    ...  \n",
            "18391                        In your head they are dying    Sad  \n",
            "18393                         In your head, in your head    Sad  \n",
            "18394                             Zombie, zombie, zombie    Sad  \n",
            "18395                  What's in your head, in your head    Sad  \n",
            "18396                             Zombie, zombie, zombie    Sad  \n",
            "\n",
            "[16152 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train['data']\n",
        "y_train = df_train['label']\n",
        "\n",
        "X_test = df_test['data']\n",
        "y_test = df_test['label']"
      ],
      "metadata": {
        "id": "aMK2SockSy0f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "X_train = df_train['data'].values \n",
        "\n",
        "y_train = df_train['label'].values\n",
        "\n",
        "print('before: %s ...' %y_train[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "print('after: %s ...' %y_train[:5])\n",
        "\n",
        "X_test = df_test['data'].values \n",
        "\n",
        "y_test = df_test['label'].values\n",
        "\n",
        "print('before: %s ...' %y_test[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(y_test)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "print('after: %s ...' %y_test[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgab3ExgTN6s",
        "outputId": "1d89eea3-d3fd-4601-c851-215ee8e01586"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: ['Angry' 'Angry' 'Angry' 'Angry' 'Angry'] ...\n",
            "after: [0 0 0 0 0] ...\n",
            "before: ['Angry' 'Angry' 'Angry' 'Angry' 'Angry'] ...\n",
            "after: [0 0 0 0 0] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "stop_words = pickle.load(open('./stopwords.p', 'rb'))\n",
        "semantic_words = pickle.load(open('./semantic_words_py34.p', 'rb'))"
      ],
      "metadata": {
        "id": "ptkx8zwAXu_1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = EnglishStemmer()\n",
        "\n",
        "# raw words\n",
        "tokenizer = lambda text: text.split()\n",
        "\n",
        "# words after Porter stemming \n",
        "tokenizer_porter = lambda text: [porter.stem(word) for word in text.split()]\n",
        "\n",
        "# Words after Snowball stemming\n",
        "tokenizer_snowball = lambda text: [snowball.stem(word) for word in text.split()]\n",
        "\n",
        "# Only words that are in a list of 'positive' or 'negative' words ('whitelist')\n",
        "# http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\n",
        "tokenizer_whitelist = lambda text: [word for word in text.split() if word in semantic_words]\n",
        "\n",
        "# Porter-stemmed words in whitelist\n",
        "tokenizer_porter_wl = lambda text: [porter.stem(word) for word in text.split() if word in semantic_words]\n",
        "\n",
        "# Snowball-stemmed words in whitelist\n",
        "tokenizer_snowball_wl = lambda text: [snowball.stem(word) for word in text.split() if word in semantic_words]"
      ],
      "metadata": {
        "id": "k_UjqgeXYKQf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "\n",
        "vect_1 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "vect_2 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter)\n",
        "    \n",
        "vect_3 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball)  \n",
        "\n",
        "vect_4 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_whitelist)  \n",
        "vect_5 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter_wl)\n",
        "\n",
        "vect_6 = CountVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball_wl)\n",
        "\n",
        "vect_7 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "vect_8 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter)\n",
        "    \n",
        "vect_9 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball)\n",
        "\n",
        "vect_10 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_whitelist)    \n",
        "vect_11 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_porter_wl)\n",
        "\n",
        "vect_12 = TfidfVectorizer(binary=False,\n",
        "                         stop_words=stop_words,\n",
        "                         ngram_range=(1,1),\n",
        "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                         tokenizer=tokenizer_snowball_wl)\n",
        "\n",
        "\n",
        "pipelines = []\n",
        "vectorizers = [vect_1, vect_2, vect_3, vect_4, vect_5, vect_6, vect_7, vect_8, vect_9, vect_10, vect_11, vect_12]\n",
        "for v in vectorizers:\n",
        "    pipelines.append(Pipeline([('vect', v),\n",
        "                               ('dense', DenseTransformer()),\n",
        "                               ('clf', RandomForestClassifier(n_estimators=100))]))"
      ],
      "metadata": {
        "id": "pWE2jQPHYP9O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Vocabulary sizes\\n')\n",
        "labels = ['CountVec', 'CountVec porter', 'CountVec snowball', 'CountVec wl', 'CountVec porter+wl','CountVec snowball+wl',\n",
        "          'TfidfVec', 'TfidfVec porter', 'TfidfVec snowball', 'TfidfVec wl', 'TfidfVec porter+wl','TfidfVec snowball+wl',]\n",
        "X_train = df_train['data'].values.astype('U')\n",
        "#print(X_train)\n",
        "for label, v in zip(labels, vectorizers):\n",
        "    v.fit(X_train)\n",
        "    print('%s: %s' % (label, len(v.vocabulary_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fR4b5VjZvp5",
        "outputId": "569dc1cf-7d9b-486e-b36c-b82abdd949f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary sizes\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVec: 5788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVec porter: 4474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVec snowball: 4457\n",
            "CountVec wl: 1013\n",
            "CountVec porter+wl: 831\n",
            "CountVec snowball+wl: 824\n",
            "TfidfVec: 5788\n",
            "TfidfVec porter: 4474\n",
            "TfidfVec snowball: 4457\n",
            "TfidfVec wl: 1013\n",
            "TfidfVec porter+wl: 831\n",
            "TfidfVec snowball+wl: 824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "labels = ['CountVec', 'CountVec porter', 'CountVec snowball', 'CountVec wl', 'CountVec porter+wl','CountVec snowball+wl',\n",
        "          'TfidfVec', 'TfidfVec porter', 'TfidfVec snowball', 'TfidfVec wl', 'TfidfVec porter+wl','TfidfVec snowball+wl',]\n",
        "\n",
        "\n",
        "\n",
        "d = {'Data':labels,\n",
        "     'Accuracy (%)':[],}\n",
        "\n",
        "for i,clf in enumerate(pipelines):\n",
        "    #if i >= 6:\n",
        "      #break\n",
        "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, scoring='accuracy', cv=5)\n",
        "    print('clf %s, %s: %s' % (i+1, labels[i], scores.mean()*100))\n",
        "    d['Accuracy (%)'].append('%0.2f (+/- %0.2f)' % (scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzaQxmuTG_lq",
        "outputId": "b464d65f-f1ec-4db8-8bbf-d95dbbf9ea5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 1, CountVec: 30.71733764822373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 2, CountVec porter: 31.341563825762154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 3, CountVec snowball: 31.080538717286128\n",
            "clf 4, CountVec wl: 30.881934616530216\n",
            "clf 5, CountVec porter+wl: 30.541465613704606\n",
            "clf 6, CountVec snowball+wl: 30.717365018797143\n",
            "clf 7, TfidfVec: 32.92475346358506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 8, TfidfVec porter: 33.57741605686639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', 'themselv', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 9, TfidfVec snowball: 33.236898753028875\n",
            "clf 10, TfidfVec wl: 31.36426852142552\n",
            "clf 11, TfidfVec porter+wl: 30.93864966471048\n",
            "clf 12, TfidfVec snowball+wl: 30.904636092126132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_perform = pd.DataFrame(d)\n",
        "df_perform = df_perform['Accuracy (%)']\n",
        "df_perform.index=(labels)\n",
        "df_perform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBuQ1OUgdDoH",
        "outputId": "0fb6be73-92d6-4df5-8d87-0ee5dfb930e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVec                30.72 (+/- 2.27)\n",
              "CountVec porter         31.34 (+/- 2.93)\n",
              "CountVec snowball       31.08 (+/- 2.30)\n",
              "CountVec wl             30.88 (+/- 0.66)\n",
              "CountVec porter+wl      30.54 (+/- 0.86)\n",
              "CountVec snowball+wl    30.72 (+/- 0.86)\n",
              "TfidfVec                32.92 (+/- 1.92)\n",
              "TfidfVec porter         33.58 (+/- 2.59)\n",
              "TfidfVec snowball       33.24 (+/- 2.01)\n",
              "TfidfVec wl             31.36 (+/- 0.84)\n",
              "TfidfVec porter+wl      30.94 (+/- 0.76)\n",
              "TfidfVec snowball+wl    30.90 (+/- 1.04)\n",
              "Name: Accuracy (%), dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect = TfidfVectorizer(binary=False,\n",
        "                       stop_words=stop_words,\n",
        "                       ngram_range=(1,1),\n",
        "                       preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
        "                       tokenizer=lambda text: [porter.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "MIRg8UUe5Nuc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "pipe_1 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=50))])\n",
        "\n",
        "pipe_2 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=100))])\n",
        "\n",
        "pipe_3 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=200))])\n",
        "\n",
        "pipe_4 = Pipeline([\n",
        "                ('vect',   vect),\n",
        "                ('dense', DenseTransformer()),\n",
        "                ('clf', RandomForestClassifier(n_estimators=400))])\n",
        "\n",
        "labels = [50, 100, 200, 400]\n",
        "\n",
        "for i,clf in enumerate([pipe_1, pipe_2, pipe_3, pipe_4]):\n",
        "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, scoring='accuracy', cv=5)\n",
        "    print('clf %s, %s: %0.2f (+/- %0.2f)' % (i+1, labels[i], scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-NTve-U5hf6",
        "outputId": "04385f7d-03b5-43a9-cf6e-64d4ede81abe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 1, 50: 32.83 (+/- 2.48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 2, 100: 33.17 (+/- 2.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 3, 200: 33.17 (+/- 2.09)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clf 4, 400: 33.33 (+/- 2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_feat = vect.fit_transform(X_train, y_train)\n",
        "X_train_feat = X_train_feat.toarray()\n",
        "\n",
        "print(X_train_feat)"
      ],
      "metadata": {
        "id": "V2QHBS1F580c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84686984-d571-47ce-b9cf-74ee1fdd48fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "clf_2 = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "\n",
        "tuned_parameters = [\n",
        "  {'criterion': ['gini', 'entropy'], \n",
        "   'max_features': ['auto', 'log2', 'sqrt'],\n",
        "   'min_samples_split':[2,3], \n",
        "   'min_samples_leaf':[1,2]},\n",
        " ]\n",
        "\n",
        "\n",
        "grid_search_1 = GridSearchCV(clf_2, \n",
        "                           tuned_parameters, \n",
        "                           n_jobs=1, \n",
        "                           scoring='accuracy',\n",
        "                           cv=5\n",
        "                )\n",
        "\n",
        "grid_search_1.fit(X_train_feat, y_train)\n",
        "\n",
        "df_gs = pd.DataFrame(grid_search_1.cv_results_)\n",
        "print(df_gs)\n",
        "\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print()\n",
        "print(grid_search_1.best_estimator_)\n",
        "print()\n",
        "print(\"Grid scores on development set:\")\n",
        "print()\n",
        "\n",
        "#for params, mean_score, scores in df_gv:\n",
        "    #print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "            #% (mean_score, scores.std() / 2, params))"
      ],
      "metadata": {
        "id": "A5nSJ0Ll6BA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96433387-cf4e-4924-e57f-b37f4e1e0afb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
            "0      167.510064      6.054650         1.187004        0.070274   \n",
            "1      163.459091      4.682669         1.140103        0.035122   \n",
            "2      142.986353      2.819247         0.929640        0.051935   \n",
            "3      144.480574      4.714326         0.924442        0.015381   \n",
            "4       65.799285      1.385040         1.405238        0.021545   \n",
            "5       62.795057      1.001090         1.399284        0.029284   \n",
            "6       20.170483      0.068209         0.519298        0.010660   \n",
            "7       20.780796      1.067379         0.528959        0.024659   \n",
            "8      164.500692      5.269845         1.138344        0.023134   \n",
            "9      162.918033      4.017413         1.131404        0.044386   \n",
            "10     142.933088      4.553183         0.915286        0.017867   \n",
            "11     150.396967      3.755187         0.960262        0.035249   \n",
            "12     153.676237      3.535345         1.128734        0.167482   \n",
            "13     152.701891      3.750784         1.091205        0.072915   \n",
            "14     138.107998      3.168510         0.878001        0.029813   \n",
            "15     136.675899      4.450491         0.876882        0.040714   \n",
            "16      64.042655      0.473545         1.349234        0.024013   \n",
            "17      59.818531      1.074438         1.294043        0.020746   \n",
            "18      20.307083      1.780978         0.512310        0.032138   \n",
            "19      20.539278      0.956972         0.506414        0.020372   \n",
            "20     147.122564      2.924252         1.013421        0.032922   \n",
            "21     145.824419      2.897254         1.021904        0.029750   \n",
            "22     133.296599      3.126400         0.854051        0.030124   \n",
            "23     134.650650      4.618714         0.859294        0.019500   \n",
            "\n",
            "   param_criterion param_max_features param_min_samples_leaf  \\\n",
            "0             gini               auto                      1   \n",
            "1             gini               auto                      1   \n",
            "2             gini               auto                      2   \n",
            "3             gini               auto                      2   \n",
            "4             gini               log2                      1   \n",
            "5             gini               log2                      1   \n",
            "6             gini               log2                      2   \n",
            "7             gini               log2                      2   \n",
            "8             gini               sqrt                      1   \n",
            "9             gini               sqrt                      1   \n",
            "10            gini               sqrt                      2   \n",
            "11            gini               sqrt                      2   \n",
            "12         entropy               auto                      1   \n",
            "13         entropy               auto                      1   \n",
            "14         entropy               auto                      2   \n",
            "15         entropy               auto                      2   \n",
            "16         entropy               log2                      1   \n",
            "17         entropy               log2                      1   \n",
            "18         entropy               log2                      2   \n",
            "19         entropy               log2                      2   \n",
            "20         entropy               sqrt                      1   \n",
            "21         entropy               sqrt                      1   \n",
            "22         entropy               sqrt                      2   \n",
            "23         entropy               sqrt                      2   \n",
            "\n",
            "   param_min_samples_split                                             params  \\\n",
            "0                        2  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "1                        3  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "2                        2  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "3                        3  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "4                        2  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "5                        3  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "6                        2  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "7                        3  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "8                        2  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "9                        3  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "10                       2  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "11                       3  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "12                       2  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "13                       3  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "14                       2  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "15                       3  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "16                       2  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "17                       3  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "18                       2  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "19                       3  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "20                       2  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "21                       3  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "22                       2  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "23                       3  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "\n",
            "    split0_test_score  split1_test_score  split2_test_score  \\\n",
            "0            0.362837           0.319716           0.350454   \n",
            "1            0.358014           0.307801           0.340238   \n",
            "2            0.357163           0.342128           0.348751   \n",
            "3            0.359433           0.332482           0.348184   \n",
            "4            0.363404           0.328794           0.349603   \n",
            "5            0.361986           0.332766           0.348184   \n",
            "6            0.339007           0.333901           0.356981   \n",
            "7            0.352057           0.343830           0.365210   \n",
            "8            0.363404           0.322553           0.343360   \n",
            "9            0.357730           0.310071           0.356697   \n",
            "10           0.355461           0.334184           0.347049   \n",
            "11           0.361418           0.346383           0.344211   \n",
            "12           0.352908           0.326241           0.336549   \n",
            "13           0.350922           0.315461           0.342509   \n",
            "14           0.345248           0.335603           0.337117   \n",
            "15           0.361986           0.333333           0.339671   \n",
            "16           0.350355           0.335035           0.353008   \n",
            "17           0.358298           0.334752           0.359818   \n",
            "18           0.343262           0.343546           0.353859   \n",
            "19           0.340993           0.340993           0.360102   \n",
            "20           0.353759           0.305816           0.338536   \n",
            "21           0.353759           0.321418           0.339103   \n",
            "22           0.357447           0.339291           0.349035   \n",
            "23           0.353759           0.343546           0.345346   \n",
            "\n",
            "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
            "0            0.374291           0.295403         0.340540        0.029002   \n",
            "1            0.390465           0.303916         0.340087        0.032280   \n",
            "2            0.375709           0.320091         0.348768        0.018236   \n",
            "3            0.379682           0.318956         0.347747        0.021064   \n",
            "4            0.371169           0.321793         0.346953        0.019115   \n",
            "5            0.383939           0.315551         0.348485        0.023542   \n",
            "6            0.372588           0.336266         0.347748        0.014848   \n",
            "7            0.376844           0.345914         0.356771        0.012504   \n",
            "8            0.374291           0.294552         0.339632        0.028650   \n",
            "9            0.367764           0.295970         0.337647        0.028881   \n",
            "10           0.385641           0.315834         0.347634        0.023230   \n",
            "11           0.387628           0.311578         0.350244        0.024774   \n",
            "12           0.359818           0.297389         0.334581        0.022043   \n",
            "13           0.367196           0.293984         0.334014        0.026099   \n",
            "14           0.380250           0.328604         0.345364        0.018228   \n",
            "15           0.364075           0.326050         0.345023        0.015336   \n",
            "16           0.368048           0.309591         0.343207        0.019804   \n",
            "17           0.359818           0.311010         0.344739        0.019370   \n",
            "18           0.367764           0.343076         0.350302        0.009643   \n",
            "19           0.376277           0.339671         0.351607        0.014481   \n",
            "20           0.364075           0.298524         0.332142        0.025889   \n",
            "21           0.371737           0.305619         0.338327        0.023284   \n",
            "22           0.377412           0.316969         0.348031        0.019971   \n",
            "23           0.375709           0.322077         0.348088        0.017319   \n",
            "\n",
            "    rank_test_score  \n",
            "0                17  \n",
            "1                18  \n",
            "2                 5  \n",
            "3                10  \n",
            "4                12  \n",
            "5                 6  \n",
            "6                 9  \n",
            "7                 1  \n",
            "8                19  \n",
            "9                21  \n",
            "10               11  \n",
            "11                4  \n",
            "12               22  \n",
            "13               23  \n",
            "14               13  \n",
            "15               14  \n",
            "16               16  \n",
            "17               15  \n",
            "18                3  \n",
            "19                2  \n",
            "20               24  \n",
            "21               20  \n",
            "22                8  \n",
            "23                7  \n",
            "Best parameters set found on development set:\n",
            "\n",
            "RandomForestClassifier(max_features='log2', min_samples_leaf=2,\n",
            "                       min_samples_split=3)\n",
            "\n",
            "Grid scores on development set:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_gs)"
      ],
      "metadata": {
        "id": "XFYJhdfFbriz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c43e076-e5e2-47e5-82c4-9011d0b67b92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
            "0      167.510064      6.054650         1.187004        0.070274   \n",
            "1      163.459091      4.682669         1.140103        0.035122   \n",
            "2      142.986353      2.819247         0.929640        0.051935   \n",
            "3      144.480574      4.714326         0.924442        0.015381   \n",
            "4       65.799285      1.385040         1.405238        0.021545   \n",
            "5       62.795057      1.001090         1.399284        0.029284   \n",
            "6       20.170483      0.068209         0.519298        0.010660   \n",
            "7       20.780796      1.067379         0.528959        0.024659   \n",
            "8      164.500692      5.269845         1.138344        0.023134   \n",
            "9      162.918033      4.017413         1.131404        0.044386   \n",
            "10     142.933088      4.553183         0.915286        0.017867   \n",
            "11     150.396967      3.755187         0.960262        0.035249   \n",
            "12     153.676237      3.535345         1.128734        0.167482   \n",
            "13     152.701891      3.750784         1.091205        0.072915   \n",
            "14     138.107998      3.168510         0.878001        0.029813   \n",
            "15     136.675899      4.450491         0.876882        0.040714   \n",
            "16      64.042655      0.473545         1.349234        0.024013   \n",
            "17      59.818531      1.074438         1.294043        0.020746   \n",
            "18      20.307083      1.780978         0.512310        0.032138   \n",
            "19      20.539278      0.956972         0.506414        0.020372   \n",
            "20     147.122564      2.924252         1.013421        0.032922   \n",
            "21     145.824419      2.897254         1.021904        0.029750   \n",
            "22     133.296599      3.126400         0.854051        0.030124   \n",
            "23     134.650650      4.618714         0.859294        0.019500   \n",
            "\n",
            "   param_criterion param_max_features param_min_samples_leaf  \\\n",
            "0             gini               auto                      1   \n",
            "1             gini               auto                      1   \n",
            "2             gini               auto                      2   \n",
            "3             gini               auto                      2   \n",
            "4             gini               log2                      1   \n",
            "5             gini               log2                      1   \n",
            "6             gini               log2                      2   \n",
            "7             gini               log2                      2   \n",
            "8             gini               sqrt                      1   \n",
            "9             gini               sqrt                      1   \n",
            "10            gini               sqrt                      2   \n",
            "11            gini               sqrt                      2   \n",
            "12         entropy               auto                      1   \n",
            "13         entropy               auto                      1   \n",
            "14         entropy               auto                      2   \n",
            "15         entropy               auto                      2   \n",
            "16         entropy               log2                      1   \n",
            "17         entropy               log2                      1   \n",
            "18         entropy               log2                      2   \n",
            "19         entropy               log2                      2   \n",
            "20         entropy               sqrt                      1   \n",
            "21         entropy               sqrt                      1   \n",
            "22         entropy               sqrt                      2   \n",
            "23         entropy               sqrt                      2   \n",
            "\n",
            "   param_min_samples_split                                             params  \\\n",
            "0                        2  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "1                        3  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "2                        2  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "3                        3  {'criterion': 'gini', 'max_features': 'auto', ...   \n",
            "4                        2  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "5                        3  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "6                        2  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "7                        3  {'criterion': 'gini', 'max_features': 'log2', ...   \n",
            "8                        2  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "9                        3  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "10                       2  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "11                       3  {'criterion': 'gini', 'max_features': 'sqrt', ...   \n",
            "12                       2  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "13                       3  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "14                       2  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "15                       3  {'criterion': 'entropy', 'max_features': 'auto...   \n",
            "16                       2  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "17                       3  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "18                       2  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "19                       3  {'criterion': 'entropy', 'max_features': 'log2...   \n",
            "20                       2  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "21                       3  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "22                       2  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "23                       3  {'criterion': 'entropy', 'max_features': 'sqrt...   \n",
            "\n",
            "    split0_test_score  split1_test_score  split2_test_score  \\\n",
            "0            0.362837           0.319716           0.350454   \n",
            "1            0.358014           0.307801           0.340238   \n",
            "2            0.357163           0.342128           0.348751   \n",
            "3            0.359433           0.332482           0.348184   \n",
            "4            0.363404           0.328794           0.349603   \n",
            "5            0.361986           0.332766           0.348184   \n",
            "6            0.339007           0.333901           0.356981   \n",
            "7            0.352057           0.343830           0.365210   \n",
            "8            0.363404           0.322553           0.343360   \n",
            "9            0.357730           0.310071           0.356697   \n",
            "10           0.355461           0.334184           0.347049   \n",
            "11           0.361418           0.346383           0.344211   \n",
            "12           0.352908           0.326241           0.336549   \n",
            "13           0.350922           0.315461           0.342509   \n",
            "14           0.345248           0.335603           0.337117   \n",
            "15           0.361986           0.333333           0.339671   \n",
            "16           0.350355           0.335035           0.353008   \n",
            "17           0.358298           0.334752           0.359818   \n",
            "18           0.343262           0.343546           0.353859   \n",
            "19           0.340993           0.340993           0.360102   \n",
            "20           0.353759           0.305816           0.338536   \n",
            "21           0.353759           0.321418           0.339103   \n",
            "22           0.357447           0.339291           0.349035   \n",
            "23           0.353759           0.343546           0.345346   \n",
            "\n",
            "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
            "0            0.374291           0.295403         0.340540        0.029002   \n",
            "1            0.390465           0.303916         0.340087        0.032280   \n",
            "2            0.375709           0.320091         0.348768        0.018236   \n",
            "3            0.379682           0.318956         0.347747        0.021064   \n",
            "4            0.371169           0.321793         0.346953        0.019115   \n",
            "5            0.383939           0.315551         0.348485        0.023542   \n",
            "6            0.372588           0.336266         0.347748        0.014848   \n",
            "7            0.376844           0.345914         0.356771        0.012504   \n",
            "8            0.374291           0.294552         0.339632        0.028650   \n",
            "9            0.367764           0.295970         0.337647        0.028881   \n",
            "10           0.385641           0.315834         0.347634        0.023230   \n",
            "11           0.387628           0.311578         0.350244        0.024774   \n",
            "12           0.359818           0.297389         0.334581        0.022043   \n",
            "13           0.367196           0.293984         0.334014        0.026099   \n",
            "14           0.380250           0.328604         0.345364        0.018228   \n",
            "15           0.364075           0.326050         0.345023        0.015336   \n",
            "16           0.368048           0.309591         0.343207        0.019804   \n",
            "17           0.359818           0.311010         0.344739        0.019370   \n",
            "18           0.367764           0.343076         0.350302        0.009643   \n",
            "19           0.376277           0.339671         0.351607        0.014481   \n",
            "20           0.364075           0.298524         0.332142        0.025889   \n",
            "21           0.371737           0.305619         0.338327        0.023284   \n",
            "22           0.377412           0.316969         0.348031        0.019971   \n",
            "23           0.375709           0.322077         0.348088        0.017319   \n",
            "\n",
            "    rank_test_score  \n",
            "0                17  \n",
            "1                18  \n",
            "2                 5  \n",
            "3                10  \n",
            "4                12  \n",
            "5                 6  \n",
            "6                 9  \n",
            "7                 1  \n",
            "8                19  \n",
            "9                21  \n",
            "10               11  \n",
            "11                4  \n",
            "12               22  \n",
            "13               23  \n",
            "14               13  \n",
            "15               14  \n",
            "16               16  \n",
            "17               15  \n",
            "18                3  \n",
            "19                2  \n",
            "20               24  \n",
            "21               20  \n",
            "22                8  \n",
            "23                7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "acc_scorer = metrics.make_scorer(metrics.accuracy_score, greater_is_better=True)\n",
        "pre_scorer = metrics.make_scorer(metrics.precision_score, greater_is_better=True, average = 'macro')\n",
        "rec_scorer = metrics.make_scorer(metrics.recall_score, greater_is_better=True, average = 'macro')\n",
        "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, average = 'macro')"
      ],
      "metadata": {
        "id": "qRATmB5t6K07"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Train CountVec', 'Train CountVec porter', 'Train CountVec snowball', 'Train CountVec wl', \n",
        "          'Train CountVec porter+wl','Train CountVec snowball+wl',\n",
        "          'Train TfidfVec', 'Train TfidfVec porter', 'Train TfidfVec snowball', 'Train TfidfVec wl', \n",
        "          'Train TfidfVec porter+wl','Train TfidfVec snowball+wl',\n",
        "          'Test CountVec', 'Test CountVec porter', 'Test CountVec snowball', 'Test CountVec wl', \n",
        "          'Test CountVec porter+wl','Test CountVec snowball+wl',\n",
        "          'Test TfidfVec', 'Test TfidfVec porter', 'Test TfidfVec snowball', 'Test TfidfVec wl', \n",
        "          'Test TfidfVec porter+wl','Test TfidfVec snowball+wl',]\n",
        "\n",
        "d = {'Data':labels,\n",
        "     'ACC (%)':[],\n",
        "     'PRE (%)':[],\n",
        "     'REC (%)':[],\n",
        "     'F1 (%)':[],\n",
        "}\n",
        "\n",
        "for clf in pipelines:\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "for clf in pipelines:\n",
        "\n",
        "    d['ACC (%)'].append(acc_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "    d['PRE (%)'].append(pre_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "    d['REC (%)'].append(rec_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "    d['F1 (%)'].append(f1_scorer(estimator=clf, X=X_train, y_true=y_train))\n",
        "\n",
        "for clf in pipelines:\n",
        "\n",
        "    d['ACC (%)'].append(acc_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
        "    d['PRE (%)'].append(pre_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
        "    d['REC (%)'].append(rec_scorer(estimator=clf, X=X_test, y_true=y_test))\n",
        "    d['F1 (%)'].append(f1_scorer(estimator=clf, X=X_test, y_true=y_test))"
      ],
      "metadata": {
        "id": "chO4aycn6MKV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d)\n",
        "print(len(d['Data']))\n",
        "print(len(d['ACC (%)']))\n",
        "print(len(d['PRE (%)']))\n",
        "print(len(d['REC (%)']))\n",
        "print(len(d['F1 (%)']))"
      ],
      "metadata": {
        "id": "QVDJxtWiRhzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac83bb0-518f-4d28-e277-d46d7b87a472"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Data': ['Train CountVec', 'Train CountVec porter', 'Train CountVec snowball', 'Train CountVec wl', 'Train CountVec porter+wl', 'Train CountVec snowball+wl', 'Train TfidfVec', 'Train TfidfVec porter', 'Train TfidfVec snowball', 'Train TfidfVec wl', 'Train TfidfVec porter+wl', 'Train TfidfVec snowball+wl', 'Test CountVec', 'Test CountVec porter', 'Test CountVec snowball', 'Test CountVec wl', 'Test CountVec porter+wl', 'Test CountVec snowball+wl', 'Test TfidfVec', 'Test TfidfVec porter', 'Test TfidfVec snowball', 'Test TfidfVec wl', 'Test TfidfVec porter+wl', 'Test TfidfVec snowball+wl'], 'ACC (%)': [0.9435364884803087, 0.9447849279309953, 0.9423447962773805, 0.4288956985586199, 0.41936216093519463, 0.41919191919191917, 0.9402451481103167, 0.9409261150834185, 0.9385994779253206, 0.4261150834184542, 0.4156168425831347, 0.41550334808761774, 0.29160475482912335, 0.2920381376919267, 0.29240960871718674, 0.2638682516097078, 0.2642397226349678, 0.26492075284794453, 0.3070208023774146, 0.31785537394749874, 0.31766963843486873, 0.2616394254581476, 0.266840019811788, 0.2669638434868747], 'PRE (%)': [0.9435394245052953, 0.9431057012099325, 0.940902413083516, 0.547213192049447, 0.5310116082305211, 0.5341293414695112, 0.9401685496090011, 0.9390311571903189, 0.9372867037147854, 0.5425491075899995, 0.533921573591154, 0.5334201489740096, 0.291117473002731, 0.2903303332681698, 0.29079500838815064, 0.2683950859386803, 0.2645300471459797, 0.264562802293353, 0.30119178923624795, 0.30974915994878016, 0.30903564710634207, 0.2710848699823234, 0.2740633360387191, 0.2766519299861958], 'REC (%)': [0.9409134225377165, 0.9428007403242229, 0.9403292961197528, 0.3979663673717815, 0.3881210545996868, 0.38780447679318125, 0.9374802988999621, 0.9390592923500287, 0.936591720081622, 0.3955546130572302, 0.3837076194870458, 0.3834897970205491, 0.29405770435119793, 0.29113655099235514, 0.2918734899048936, 0.26874033788567636, 0.2689659376571857, 0.26918714918588327, 0.3082497122791499, 0.3164099141808103, 0.315308943061753, 0.26848328020134665, 0.2711437105788009, 0.27160253903684073], 'F1 (%)': [0.942022355148354, 0.942852514351446, 0.940430223045491, 0.3814552138179357, 0.36823567101534505, 0.36738864320205245, 0.9386634939501189, 0.9389260970696807, 0.9366628256056796, 0.3781187608616063, 0.3609969474685434, 0.3604297217583734, 0.2878611887309941, 0.28952019396245604, 0.29028882769856756, 0.19785341819917557, 0.19622738417464772, 0.19595688364218092, 0.2979885942420148, 0.31039579327597216, 0.3098976507023575, 0.19779633556886905, 0.19831902162929804, 0.19912536075310944]}\n",
            "24\n",
            "24\n",
            "24\n",
            "24\n",
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('precision', 2)\n",
        "\n",
        "df_perform = pd.DataFrame(d)\n",
        "df_perform = df_perform[['ACC (%)', 'PRE (%)', 'REC (%)', 'F1 (%)']]\n",
        "df_perform.index=(labels)\n",
        "df_perform = df_perform*100\n",
        "df_perform = np.round(df_perform, decimals=2)\n",
        "df_perform"
      ],
      "metadata": {
        "id": "vXcqfUyM6hXb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "6ec24fbf-ddab-4a89-a772-54b0b92efcc8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            ACC (%)  PRE (%)  REC (%)  F1 (%)\n",
              "Train CountVec                94.35    94.35    94.09   94.20\n",
              "Train CountVec porter         94.48    94.31    94.28   94.29\n",
              "Train CountVec snowball       94.23    94.09    94.03   94.04\n",
              "Train CountVec wl             42.89    54.72    39.80   38.15\n",
              "Train CountVec porter+wl      41.94    53.10    38.81   36.82\n",
              "Train CountVec snowball+wl    41.92    53.41    38.78   36.74\n",
              "Train TfidfVec                94.02    94.02    93.75   93.87\n",
              "Train TfidfVec porter         94.09    93.90    93.91   93.89\n",
              "Train TfidfVec snowball       93.86    93.73    93.66   93.67\n",
              "Train TfidfVec wl             42.61    54.25    39.56   37.81\n",
              "Train TfidfVec porter+wl      41.56    53.39    38.37   36.10\n",
              "Train TfidfVec snowball+wl    41.55    53.34    38.35   36.04\n",
              "Test CountVec                 29.16    29.11    29.41   28.79\n",
              "Test CountVec porter          29.20    29.03    29.11   28.95\n",
              "Test CountVec snowball        29.24    29.08    29.19   29.03\n",
              "Test CountVec wl              26.39    26.84    26.87   19.79\n",
              "Test CountVec porter+wl       26.42    26.45    26.90   19.62\n",
              "Test CountVec snowball+wl     26.49    26.46    26.92   19.60\n",
              "Test TfidfVec                 30.70    30.12    30.82   29.80\n",
              "Test TfidfVec porter          31.79    30.97    31.64   31.04\n",
              "Test TfidfVec snowball        31.77    30.90    31.53   30.99\n",
              "Test TfidfVec wl              26.16    27.11    26.85   19.78\n",
              "Test TfidfVec porter+wl       26.68    27.41    27.11   19.83\n",
              "Test TfidfVec snowball+wl     26.70    27.67    27.16   19.91"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e4222e4-9f0c-4faf-870e-f6b186d8ef48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACC (%)</th>\n",
              "      <th>PRE (%)</th>\n",
              "      <th>REC (%)</th>\n",
              "      <th>F1 (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Train CountVec</th>\n",
              "      <td>94.35</td>\n",
              "      <td>94.35</td>\n",
              "      <td>94.09</td>\n",
              "      <td>94.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train CountVec porter</th>\n",
              "      <td>94.48</td>\n",
              "      <td>94.31</td>\n",
              "      <td>94.28</td>\n",
              "      <td>94.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train CountVec snowball</th>\n",
              "      <td>94.23</td>\n",
              "      <td>94.09</td>\n",
              "      <td>94.03</td>\n",
              "      <td>94.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train CountVec wl</th>\n",
              "      <td>42.89</td>\n",
              "      <td>54.72</td>\n",
              "      <td>39.80</td>\n",
              "      <td>38.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train CountVec porter+wl</th>\n",
              "      <td>41.94</td>\n",
              "      <td>53.10</td>\n",
              "      <td>38.81</td>\n",
              "      <td>36.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train CountVec snowball+wl</th>\n",
              "      <td>41.92</td>\n",
              "      <td>53.41</td>\n",
              "      <td>38.78</td>\n",
              "      <td>36.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train TfidfVec</th>\n",
              "      <td>94.02</td>\n",
              "      <td>94.02</td>\n",
              "      <td>93.75</td>\n",
              "      <td>93.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train TfidfVec porter</th>\n",
              "      <td>94.09</td>\n",
              "      <td>93.90</td>\n",
              "      <td>93.91</td>\n",
              "      <td>93.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train TfidfVec snowball</th>\n",
              "      <td>93.86</td>\n",
              "      <td>93.73</td>\n",
              "      <td>93.66</td>\n",
              "      <td>93.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train TfidfVec wl</th>\n",
              "      <td>42.61</td>\n",
              "      <td>54.25</td>\n",
              "      <td>39.56</td>\n",
              "      <td>37.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train TfidfVec porter+wl</th>\n",
              "      <td>41.56</td>\n",
              "      <td>53.39</td>\n",
              "      <td>38.37</td>\n",
              "      <td>36.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train TfidfVec snowball+wl</th>\n",
              "      <td>41.55</td>\n",
              "      <td>53.34</td>\n",
              "      <td>38.35</td>\n",
              "      <td>36.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test CountVec</th>\n",
              "      <td>29.16</td>\n",
              "      <td>29.11</td>\n",
              "      <td>29.41</td>\n",
              "      <td>28.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test CountVec porter</th>\n",
              "      <td>29.20</td>\n",
              "      <td>29.03</td>\n",
              "      <td>29.11</td>\n",
              "      <td>28.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test CountVec snowball</th>\n",
              "      <td>29.24</td>\n",
              "      <td>29.08</td>\n",
              "      <td>29.19</td>\n",
              "      <td>29.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test CountVec wl</th>\n",
              "      <td>26.39</td>\n",
              "      <td>26.84</td>\n",
              "      <td>26.87</td>\n",
              "      <td>19.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test CountVec porter+wl</th>\n",
              "      <td>26.42</td>\n",
              "      <td>26.45</td>\n",
              "      <td>26.90</td>\n",
              "      <td>19.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test CountVec snowball+wl</th>\n",
              "      <td>26.49</td>\n",
              "      <td>26.46</td>\n",
              "      <td>26.92</td>\n",
              "      <td>19.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test TfidfVec</th>\n",
              "      <td>30.70</td>\n",
              "      <td>30.12</td>\n",
              "      <td>30.82</td>\n",
              "      <td>29.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test TfidfVec porter</th>\n",
              "      <td>31.79</td>\n",
              "      <td>30.97</td>\n",
              "      <td>31.64</td>\n",
              "      <td>31.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test TfidfVec snowball</th>\n",
              "      <td>31.77</td>\n",
              "      <td>30.90</td>\n",
              "      <td>31.53</td>\n",
              "      <td>30.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test TfidfVec wl</th>\n",
              "      <td>26.16</td>\n",
              "      <td>27.11</td>\n",
              "      <td>26.85</td>\n",
              "      <td>19.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test TfidfVec porter+wl</th>\n",
              "      <td>26.68</td>\n",
              "      <td>27.41</td>\n",
              "      <td>27.11</td>\n",
              "      <td>19.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test TfidfVec snowball+wl</th>\n",
              "      <td>26.70</td>\n",
              "      <td>27.67</td>\n",
              "      <td>27.16</td>\n",
              "      <td>19.91</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e4222e4-9f0c-4faf-870e-f6b186d8ef48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7e4222e4-9f0c-4faf-870e-f6b186d8ef48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7e4222e4-9f0c-4faf-870e-f6b186d8ef48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}